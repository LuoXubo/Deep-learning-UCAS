{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自动写诗\n",
    "\n",
    "首先导入必要的库："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载数据集\n",
    "\n",
    "本次实验的数据来自chinese-poetry：https://github.com/chinese-poetry/chinese-poetry\n",
    "\n",
    "实验提供预处理过的数据集，含有57580首唐诗，每首诗限定在125词，不足125词的以```<s>```填充。数据集以npz文件形式保存，包含三个部分：\n",
    "- （1）data: 诗词数据，将诗词中的字转化为其在字典中的序号表示。\n",
    "- （2）ix2word: 序号到字的映射\n",
    "- （3）word2ix: 字到序号的映射\n",
    "\n",
    "预处理数据集的下载：[点击下载](https://yun.sfo2.digitaloceanspaces.com/pytorch_book/pytorch_book/tang.npz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def prepareData():\n",
    "    \n",
    "    # 读入预处理的数据\n",
    "    datas = np.load(\"../../Dataset/tang.npz\", allow_pickle=True)\n",
    "    data = datas['data']\n",
    "    ix2word = datas['ix2word'].item()\n",
    "    word2ix = datas['word2ix'].item()\n",
    "    \n",
    "    # 转为torch.Tensor\n",
    "    data = torch.from_numpy(data)\n",
    "    dataloader = DataLoader(data,\n",
    "                         batch_size = 16,\n",
    "                         shuffle = True,\n",
    "                         num_workers = 2)\n",
    "    \n",
    "    return dataloader, ix2word, word2ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "dataloader, ix2word, word2ix = prepareData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建模型\n",
    "\n",
    "模型包括Embedding层、LSTM层和输出层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "class PoetryModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(PoetryModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, self.hidden_dim, num_layers=2)\n",
    "        self.linear = nn.Linear(self.hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input, hidden = None):\n",
    "        seq_len, batch_size = input.size()\n",
    "        \n",
    "        if hidden is None:\n",
    "            h_0 = input.data.new(2, batch_size, self.hidden_dim).fill_(0).float()\n",
    "            c_0 = input.data.new(2, batch_size, self.hidden_dim).fill_(0).float()\n",
    "        else:\n",
    "            h_0, c_0 = hidden\n",
    "\n",
    "        embeds = self.embedding(input)\n",
    "        output, hidden = self.lstm(embeds, (h_0, c_0))\n",
    "        output = self.linear(output.view(seq_len * batch_size, -1))\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class PoetryTransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_heads, num_layers):\n",
    "        super(PoetryTransformerModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.positional_encoding = PositionalEncoding(embedding_dim)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=embedding_dim,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dim_feedforward=hidden_dim\n",
    "        )\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None, memory_mask=None):\n",
    "        # Embedding and positional encoding\n",
    "        src_emb = self.embedding(src) * math.sqrt(self.embedding.embedding_dim)\n",
    "        tgt_emb = self.embedding(tgt) * math.sqrt(self.embedding.embedding_dim)\n",
    "        src_emb = self.positional_encoding(src_emb)\n",
    "        tgt_emb = self.positional_encoding(tgt_emb)\n",
    "\n",
    "        # Transformer forward pass\n",
    "        output = self.transformer(\n",
    "            src_emb, tgt_emb,\n",
    "            src_mask=src_mask,\n",
    "            tgt_mask=tgt_mask,\n",
    "            memory_mask=memory_mask\n",
    "        )\n",
    "\n",
    "        # Final linear layer\n",
    "        output = self.linear(output)\n",
    "        return output\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "vocab_size = 125\n",
    "embedding_dim = 256\n",
    "hidden_dim = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "\n",
    "model = PoetryTransformerModel(vocab_size, embedding_dim, hidden_dim, num_heads, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoetryModel2(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(PoetryModel2, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = LSTM_LAYER\n",
    "        self.lstm_outdim = LSTM_OUTDIM\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm1 = nn.LSTM(embedding_dim, self.hidden_dim, num_layers=self.num_layers)\n",
    "        self.lstm2 = nn.LSTM(self.hidden_dim, self.lstm_outdim, num_layers=self.num_layers, batch_first=True)\n",
    "        self.fc = nn.Sequential(nn.Linear(self.lstm_outdim, 2048),\n",
    "                                nn.Tanh(),\n",
    "                                nn.Linear(2048, vocab_size))\n",
    "        self.dropout = nn.Dropout(0.6)\n",
    "\n",
    "    def forward(self, input, hidden1=None, hidden2=None):\n",
    "        seq_len, batch_size = input.size()\n",
    "        if hidden1 is None or hidden2 is None:\n",
    "            h_0 = input.data.new(self.num_layers, batch_size, self.hidden_dim).fill_(0).float()\n",
    "            c_0 = input.data.new(self.num_layers, batch_size, self.hidden_dim).fill_(0).float()\n",
    "            h_1 = input.data.new(self.num_layers, seq_len, self.lstm_outdim).fill_(0).float()\n",
    "            c_1 = input.data.new(self.num_layers, seq_len, self.lstm_outdim).fill_(0).float()\n",
    "        else:\n",
    "            h_0, c_0 = hidden1\n",
    "            h_1, c_1 = hidden2\n",
    "        embeds = self.embeddings(input)\n",
    "        output, hidden1 = self.lstm1(embeds, (h_0, c_0))\n",
    "        output, hidden2 = self.lstm2(output, (h_1, c_1))\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc(output.reshape(seq_len * batch_size, -1))\n",
    "        return output, hidden1, hidden2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 设置超参数\n",
    "learning_rate = 5e-3       # 学习率\n",
    "embedding_dim = 128        # 嵌入层维度\n",
    "hidden_dim = 256           # 隐藏层维度\n",
    "model_path = None          # 预训练模型路径\n",
    "epochs = 20                # 训练轮数\n",
    "verbose = True             # 打印训练过程\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def train(dataloader, ix2word, word2ix):\n",
    "\n",
    "    vocab_size = 125\n",
    "    embedding_dim = 256\n",
    "    hidden_dim = 512\n",
    "    num_heads = 8\n",
    "    num_layers = 6\n",
    "\n",
    "    model = PoetryTransformerModel(vocab_size, embedding_dim, hidden_dim, num_heads, num_layers)\n",
    "    # 配置模型，是否继续上一次的训练\n",
    "    # model = PoetryModel(len(word2ix), embedding_dim, hidden_dim)\n",
    "    if model_path:\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "    model.to(device)\n",
    "    \n",
    "    # 设置优化器\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "    # 设置损失函数\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # 定义训练过程\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, data in enumerate(dataloader):\n",
    "            data = data.long().transpose(1, 0).contiguous()\n",
    "            data = data.to(device)\n",
    "            input, target = data[:-1, :], data[1:, :]\n",
    "            output, _ = model(input)\n",
    "            loss = criterion(output, target.view(-1))\n",
    "            \n",
    "            if batch_idx % 900 == 0 & verbose:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch+1, batch_idx * len(data[1]), len(dataloader.dataset),\n",
    "                    100. * batch_idx / len(dataloader), loss.item()))\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # 保存模型\n",
    "    torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'tgt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mix2word\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword2ix\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 28\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataloader, ix2word, word2ix)\u001b[0m\n\u001b[1;32m     26\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28minput\u001b[39m, target \u001b[38;5;241m=\u001b[39m data[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :], data[\u001b[38;5;241m1\u001b[39m:, :]\n\u001b[0;32m---> 28\u001b[0m output, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, target\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m900\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m&\u001b[39m verbose:\n",
      "File \u001b[0;32m~/anaconda3/envs/prml/lib/python3.8/site-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'tgt'"
     ]
    }
   ],
   "source": [
    "train(dataloader, ix2word, word2ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生成唐诗\n",
    "\n",
    "给定几个词，根据这几个词接着生成一首完整的唐诗。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 设置超参数\n",
    "model_path = 'model.pth'        # 模型路径\n",
    "start_words = '湖光秋月两相和'  # 唐诗的第一句\n",
    "max_gen_len = 125                # 生成唐诗的最长长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "metadata": {},
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def generate(start_words, ix2word, word2ix):\n",
    "\n",
    "    # 读取模型\n",
    "    model = PoetryModel(len(word2ix), embedding_dim, hidden_dim)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(device)\n",
    "    \n",
    "    # 读取唐诗的第一句\n",
    "    results = list(start_words)\n",
    "    start_word_len = len(start_words)\n",
    "    \n",
    "    # 设置第一个词为<START>\n",
    "    input = torch.Tensor([word2ix['<START>']]).view(1, 1).long()\n",
    "    input = input.to(device)\n",
    "    hidden = None\n",
    "\n",
    "    # 生成唐诗\n",
    "    for i in range(max_gen_len):\n",
    "        output, hidden = model(input, hidden)\n",
    "        # 读取第一句\n",
    "        if i < start_word_len:\n",
    "            w = results[i]\n",
    "            input = input.data.new([word2ix[w]]).view(1, 1)\n",
    "        # 生成后面的句子\n",
    "        else:\n",
    "            top_index = output.data[0].topk(1)[1][0].item()\n",
    "            w = ix2word[top_index]\n",
    "            results.append(w)\n",
    "            input = input.data.new([top_index]).view(1, 1)\n",
    "        # 结束标志\n",
    "        if w == '<EOP>':\n",
    "            del results[-1]\n",
    "            break\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['湖', '光', '秋', '月', '两', '相', '和', '，', '水', '底', '松', '声', '闻', '擣', '衣', '。', '一', '片', '月', '明', '秋', '水', '冷', '，', '一', '声', '蝉', '啸', '月', '华', '清', '。', '一', '片', '月', '明', '秋', '水', '冷', '，', '一', '声', '蝉', '啸', '月', '华', '清', '。', '一', '片', '月', '明', '秋', '水', '绿', '，', '一', '声', '蝉', '啸', '月', '华', '清', '。', '不', '知', '何', '处', '归', '云', '去', '，', '不', '见', '春', '风', '吹', '白', '苹', '。', '春', '风', '吹', '落', '春', '风', '起', '，', '春', '风', '吹', '尽', '春', '风', '起', '。', '春', '风', '吹', '尽', '春', '风', '起', '，', '春', '风', '吹', '尽', '春', '风', '起', '。', '春', '风', '吹', '尽', '猩', '猩', '飞', '，', '玉', '筯', '双', '飞', '入']\n"
     ]
    }
   ],
   "source": [
    "results = generate(start_words, ix2word, word2ix)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生成藏头诗\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 设置超参数\n",
    "model_path = 'model.pth'                 # 模型路径\n",
    "start_words_acrostic = '湖光秋月两相和'  # 唐诗的“头”\n",
    "max_gen_len_acrostic = 125               # 生成唐诗的最长长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "metadata": {},
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def gen_acrostic(start_words, ix2word, word2ix):\n",
    "\n",
    "    # 读取模型\n",
    "    model = PoetryModel(len(word2ix), embedding_dim, hidden_dim)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(device)\n",
    "    \n",
    "    # 读取唐诗的“头”\n",
    "    results = []\n",
    "    start_word_len = len(start_words)\n",
    "    \n",
    "    # 设置第一个词为<START>\n",
    "    input = (torch.Tensor([word2ix['<START>']]).view(1, 1).long())\n",
    "    input = input.to(device)\n",
    "    hidden = None\n",
    "\n",
    "    index = 0            # 指示已生成了多少句\n",
    "    pre_word = '<START>' # 上一个词\n",
    "\n",
    "    # 生成藏头诗\n",
    "    for i in range(max_gen_len_acrostic):\n",
    "        output, hidden = model(input, hidden)\n",
    "        top_index = output.data[0].topk(1)[1][0].item()\n",
    "        w = ix2word[top_index]\n",
    "\n",
    "        # 如果遇到标志一句的结尾，喂入下一个“头”\n",
    "        if (pre_word in {u'。', u'！', '<START>'}):\n",
    "            # 如果生成的诗已经包含全部“头”，则结束\n",
    "            if index == start_word_len:\n",
    "                break\n",
    "            # 把“头”作为输入喂入模型\n",
    "            else:\n",
    "                w = start_words[index]\n",
    "                index += 1\n",
    "                input = (input.data.new([word2ix[w]])).view(1, 1)\n",
    "                \n",
    "        # 否则，把上一次预测作为下一个词输入\n",
    "        else:\n",
    "            input = (input.data.new([word2ix[w]])).view(1, 1)\n",
    "        results.append(w)\n",
    "        pre_word = w\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['湖', '上', '有', '君', '子', '，', '一', '朝', '无', '所', '求', '。', '光', '辉', '不', '可', '见', '，', '一', '旦', '不', '可', '亲', '。', '秋', '风', '吹', '白', '日', '，', '一', '夜', '无', '人', '知', '。', '月', '明', '天', '下', '静', '，', '鸟', '过', '竹', '林', '寒', '。', '两', '鬓', '不', '可', '见', '，', '一', '枝', '无', '所', '依', '。', '相', '思', '不', '可', '见', '，', '一', '夜', '不', '可', '攀', '。', '和', '风', '吹', '不', '足', '，', '秋', '草', '生', '寒', '烟', '。']\n"
     ]
    }
   ],
   "source": [
    "results_acrostic = gen_acrostic(start_words_acrostic, ix2word, word2ix)\n",
    "print(results_acrostic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
