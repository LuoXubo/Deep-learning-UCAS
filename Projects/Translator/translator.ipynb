{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6016f24",
   "metadata": {},
   "source": [
    "## 1 导入相关包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af03343c",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import math\n",
    "import torch\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import copy\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import sacrebleu\n",
    "import random\n",
    "import time\n",
    "import jieba\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # 下面老是报错 shape 不一致"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f5dbfd",
   "metadata": {},
   "source": [
    "## 2 定义dataset类\n",
    "用于传入dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d07e8aa0",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    # 创建数据集\n",
    "    def __init__(self, src, tgt):\n",
    "        \"\"\"\n",
    "        初始化\n",
    "        :param src: 源数据(经tokenizer处理后)\n",
    "        :param tgt: 目标数据(经tokenizer处理后)\n",
    "        \"\"\"\n",
    "        self.src = src\n",
    "        self.tgt = tgt\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.src[i], self.tgt[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dab7e16",
   "metadata": {},
   "source": [
    "## 3 定义tokenizer类\n",
    "其作用是读取源数据并将其处理成可供模型输入的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6744d89b",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "class Tokenizer():\n",
    "    ## 定义tokenizer,对原始数据进行处理\n",
    "    def __init__(self, en_path, ch_path, count_min=5):\n",
    "        \"\"\"\n",
    "        初始化\n",
    "        :param en_path: 英文数据路径\n",
    "        :param ch_path: 中文数据路径\n",
    "        :param count_min: 对出现次数少于这个次数的数据进行过滤\n",
    "        \"\"\"\n",
    "        self.en_path = en_path  # 英文路径\n",
    "        self.ch_path = ch_path  # 中文路径\n",
    "        self.__count_min = count_min  # 对出现次数少于这个次数的数据进行过滤\n",
    "\n",
    "        # 读取原始英文数据\n",
    "        self.en_data = self.__read_ori_data(en_path)\n",
    "        # 读取原始中文数据\n",
    "        self.ch_data = self.__read_ori_data(ch_path)\n",
    "\n",
    "        self.index_2_word = ['unK', '<pad>', '<bos>', '<eos>']\n",
    "        self.word_2_index = {'unK': 0, '<pad>': 1, '<bos>': 2, '<eos>': 3}\n",
    "\n",
    "        self.en_set = set()\n",
    "        self.en_count = {}\n",
    "\n",
    "        # 中英文字符计数\n",
    "        self.__en_count = {}\n",
    "        self.__ch_count = {}\n",
    "\n",
    "        self.__count_word()\n",
    "        self.mx_length = 40\n",
    "        # 创建英文词汇表\n",
    "        self.data_ = []\n",
    "        self.__filter_data()\n",
    "        random.shuffle(self.data_)\n",
    "        self.test = self.data_[-1000:]\n",
    "        self.data_ = self.data_[:-1000]\n",
    "\n",
    "    def __read_ori_data(self, path):\n",
    "        \"\"\"\n",
    "        读取原始数据\n",
    "        :param path: 数据路径\n",
    "        :return: 返回一个列表，每个元素是一条数据\n",
    "        \"\"\"\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            data = f.read().split('\\n')[:-1]\n",
    "        return data\n",
    "\n",
    "    def __count_word(self):\n",
    "        \"\"\"\n",
    "        统计中英文词汇表\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        le = len(self.en_data)\n",
    "        p = 0\n",
    "        # 统计英文词汇表\n",
    "        for data in self.en_data:\n",
    "            if p % 1000 == 0:\n",
    "                print('英文', p / le)\n",
    "            sentence = word_tokenize(data)\n",
    "            for sen in sentence:\n",
    "#                 sen=sen.lower()\n",
    "                if sen in self.en_set:\n",
    "                    self.en_count[sen] += 1\n",
    "                else:\n",
    "                    self.en_set.add(sen)\n",
    "                    self.en_count[sen] = 1\n",
    "            p += 1\n",
    "        for k, v in self.en_count.items():\n",
    "            if v >= self.__count_min:\n",
    "                self.word_2_index[k] = len(self.index_2_word)\n",
    "                self.index_2_word.append(k)\n",
    "            else:\n",
    "                self.word_2_index[k] = 0\n",
    "        self.en_set = set()\n",
    "        self.en_count = {}\n",
    "        p = 0\n",
    "        # 统计中文词汇表\n",
    "        for data in self.ch_data:\n",
    "            if p % 1000 == 0:\n",
    "                print('中文', p / le)\n",
    "            sentence = list(jieba.cut(data))\n",
    "            for sen in sentence:\n",
    "                if sen in self.en_set:\n",
    "                    self.en_count[sen] += 1\n",
    "                else:\n",
    "                    self.en_set.add(sen)\n",
    "                    self.en_count[sen] = 1\n",
    "            p += 1\n",
    "        # 构建词汇表\n",
    "        for k, v in self.en_count.items():\n",
    "            if v >= self.__count_min:\n",
    "                self.word_2_index[k] = len(self.index_2_word)\n",
    "                self.index_2_word.append(k)\n",
    "            else:\n",
    "                self.word_2_index[k] = 0\n",
    "\n",
    "    def __filter_data(self):\n",
    "        length = len(self.en_data)\n",
    "        for i in range(length):\n",
    "            # 0 代表英文到中文，1 代表中文到英文\n",
    "            self.data_.append([self.en_data[i], self.ch_data[i], 0])\n",
    "            self.data_.append([self.ch_data[i], self.en_data[i], 1])\n",
    "\n",
    "    def en_cut(self, data):\n",
    "        data = word_tokenize(data)\n",
    "        # 用于存放每个句子对应的编码\n",
    "        if len(data) > self.mx_length:\n",
    "            return 0, []\n",
    "        en_tokens = []\n",
    "        # 对分词结果进行遍历\n",
    "        for tk in data:\n",
    "#             x = tk.lower()\n",
    "            # 对于结果进行编码,0代表unK\n",
    "            en_tokens.append(self.word_2_index.get(tk, 0))\n",
    "        return 1, en_tokens\n",
    "\n",
    "    def ch_cut(self, data):\n",
    "        data = list(jieba.cut(data))\n",
    "#         list(data)[:-1]\n",
    "        # 用于存放每个句子对应的编码\n",
    "        if len(data) > self.mx_length:\n",
    "            return 0, []\n",
    "        en_tokens = []\n",
    "        # 对分词结果进行遍历\n",
    "        for tk in data:\n",
    "            # 对于结果进行编码,0代表unK\n",
    "            en_tokens.append(self.word_2_index.get(tk, 0))\n",
    "        return 1, en_tokens\n",
    "\n",
    "    def encode_all(self, data):\n",
    "        \"\"\"\n",
    "        对一组数据进行编码\n",
    "        :param data: data是一个数组，形状为n*3 每个元素是[src_sentence, tgt_sentence, label]，label 0 代表英文到中文，1 代表中文到英文\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        src = []\n",
    "        tgt = []\n",
    "        en_src, en_tgt, l = [], [], []\n",
    "        labels=[]\n",
    "        for i in data:\n",
    "            en_src.append(i[0])\n",
    "            en_tgt.append(i[1])\n",
    "            l.append(i[2])\n",
    "        for i in range(len(l)):\n",
    "            if l[i] == 0:\n",
    "                lab1, src_tokens = self.en_cut(en_src[i])\n",
    "                if lab1 == 0:\n",
    "                    continue\n",
    "                lab2, tgt_tokens = self.ch_cut(en_tgt[i])\n",
    "                if lab2 == 0:\n",
    "                    continue\n",
    "                src.append(src_tokens)\n",
    "                tgt.append(tgt_tokens)\n",
    "                labels.append(i)\n",
    "            else:\n",
    "                lab1, tgt_tokens = self.en_cut(en_tgt[i])\n",
    "                if lab1 == 0:\n",
    "                    continue\n",
    "                lab2, src_tokens = self.ch_cut(en_src[i])\n",
    "                if lab2 == 0:\n",
    "                    continue\n",
    "                src.append(src_tokens)\n",
    "                tgt.append(tgt_tokens)\n",
    "                labels.append(i)\n",
    "        return labels,src, tgt\n",
    "\n",
    "    def encode(self, src, l):\n",
    "        if l == 0:\n",
    "            src1 = word_tokenize(src)\n",
    "            # 用于存放每个句子对应的编码\n",
    "            en_tokens = []\n",
    "            # 对分词结果进行遍历\n",
    "            for tk in src1:\n",
    "#                 x = tk.lower()\n",
    "                # 对于结果进行编码\n",
    "                en_tokens.append(self.word_2_index.get(tk, 0))\n",
    "            return [en_tokens]\n",
    "        else:\n",
    "            src1 = list(jieba.cut(src))\n",
    "            # 用于存放每个句子对应的编码\n",
    "            en_tokens = []\n",
    "            # 对分词结果进行遍历\n",
    "            for tk in src1:\n",
    "                # 对于结果进行编码\n",
    "                en_tokens.append(self.word_2_index.get(tk, 0))\n",
    "            return [en_tokens]\n",
    "\n",
    "    def decode(self, data):\n",
    "        \"\"\"\n",
    "        数据解码\n",
    "        :param data: 这里传入一个中文的index\n",
    "        :return: 返回解码后的一个字符\n",
    "        \"\"\"\n",
    "        return self.index_2_word[data]\n",
    "\n",
    "    def __get_datasets(self, data):\n",
    "        \"\"\"\n",
    "        获取数据集\n",
    "        :return:返回DataSet类型的数据 或者 None\n",
    "        \"\"\"\n",
    "        # 将数据编码并\n",
    "        labels,src, tgt = self.encode_all(data)\n",
    "        # 返回数据集\n",
    "        return TranslationDataset(src, tgt)\n",
    "\n",
    "    def another_process(self, batch_datas):\n",
    "        \"\"\"\n",
    "        特殊处理，这里传入一个batch的数据，并对这个batch的数据进行填充，使得每一行的数据长度相同。这里填充pad 空字符  bos 开始  eos结束\n",
    "        :param batch_datas: 一个batch的数据\n",
    "        :return: 返回填充后的数据\n",
    "        \"\"\"\n",
    "        # 创建四个空字典存储数据\n",
    "        en_index, ch_index = [], []  # 中文英文索引，中文索引\n",
    "        en_len, ch_len = [], []  # 没行英文长度，每行中文长度\n",
    "\n",
    "        for en, ch in batch_datas:  # 对batch进行遍历，将所有数据的索引与长度加入四个列表\n",
    "            en_index.append(en)\n",
    "            ch_index.append(ch)\n",
    "            en_len.append(len(en))\n",
    "            ch_len.append(len(ch))\n",
    "\n",
    "        # 获取中英文的最大长度，根据这个长度对所有数据进行填充，使每行数据长度相同\n",
    "        max_en_len = max(en_len)\n",
    "        max_ch_len = max(ch_len)\n",
    "        max_len = max(max_en_len, max_ch_len + 2)\n",
    "\n",
    "        # 英文数据填充，i是原始数据，后面是填充的pad\n",
    "        en_index = [i + [self.word_2_index['<pad>']] * (max_len - len(i)) for i in en_index]\n",
    "        # 中文数据填充 先填充bos表示句子开始，后面接原始数据，最后填充eos表示句子结束，后面接pad\n",
    "        ch_index = [[self.word_2_index['<bos>']] + i + [self.word_2_index['<eos>']] +\n",
    "                    [self.word_2_index['<pad>']] * (max_len - len(i) + 1) for i in ch_index]\n",
    "\n",
    "        # 将处理后的数据转换为tensor并放到相应设备上\n",
    "        en_index = torch.tensor(en_index)\n",
    "        ch_index = torch.tensor(ch_index)\n",
    "        return en_index, ch_index\n",
    "\n",
    "    def get_dataloader(self, data, batch_size=40):\n",
    "        \"\"\"\n",
    "        获取dataloader\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # 获取数据集\n",
    "        data = self.__get_datasets(data)\n",
    "        # 返回DataLoader类型的数据\n",
    "        return DataLoader(data, batch_size=batch_size, shuffle=True, collate_fn=self.another_process)\n",
    "\n",
    "    # 获取英文词表大小\n",
    "    def get_vocab_size(self):\n",
    "        return len(self.index_2_word)\n",
    "\n",
    "    # 获取数据集大小\n",
    "    def get_dataset_size(self):\n",
    "        return len(self.en_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f6d085",
   "metadata": {},
   "source": [
    "## 4 定义batch类\n",
    "其作用是生成掩码与统计非填充字符数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f84ee93e",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    # 批次类,对每一个批次的数据进行掩码生成操作\n",
    "    def __init__(self, src, trg=None, tokenizer=None, device='cuda'):\n",
    "        \"\"\"\n",
    "        初始化函数\n",
    "        :param src: 源数据\n",
    "        :param trg: 目标数据\n",
    "        :param tokenizer: 分词器\n",
    "        :param device: 训练设备\n",
    "        \"\"\"\n",
    "        # 将输入、输出单词id表示的数据规范成整数类型并转换到训练设备上\n",
    "        src = src.to(device).long()\n",
    "        trg = trg.to(device).long()\n",
    "        self.src = src  # 源数据 (batch, seq_len)\n",
    "        self.__pad = tokenizer.word_2_index['<pad>']  # 填充字符的索引\n",
    "        # 对于当前输入的语句非空部分进行判断，这里是对源数据进行掩码操作，将填充的内容置为0\n",
    "        # 并在seq length前面增加一维，形成维度为 1×seq length 的矩阵\n",
    "        self.src_mask = (src != self.__pad).unsqueeze(-2)\n",
    "        # 如果输出目标不为空，则需要对解码器使用的目标语句进行掩码\n",
    "        if trg is not None:\n",
    "            # 解码器使用的目标输入部分\n",
    "            self.trg = trg[:, : -1]\n",
    "            # 解码器训练时应预测输出的目标结果\n",
    "            self.trg_y = trg[:, 1:]\n",
    "            # 将目标输入部分进行注意力掩码\n",
    "            self.trg_mask = self.make_std_mask(self.trg, self.__pad)\n",
    "            # 将应输出的目标结果中实际的词数进行统计\n",
    "            self.ntokens = (self.trg_y != self.__pad).data.sum()\n",
    "\n",
    "    # 掩码操作\n",
    "    @staticmethod\n",
    "    def make_std_mask(tgt, pad):\n",
    "        \"\"\"\n",
    "        生成掩码矩阵\n",
    "        :param tgt: 目标数据\n",
    "        :param pad: 填充字符的索引\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)  # 首先对pad进行掩码生成\n",
    "        # 这里对注意力进行掩码操作并与pad掩码结合起来。\n",
    "        tgt_mask = tgt_mask & Variable(subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
    "        return tgt_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4b2266",
   "metadata": {},
   "source": [
    "#### 注意力掩码生成函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26bfeea0",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"\"\"\n",
    "    注意力机制掩码生成\n",
    "    :param size: 句子长度\n",
    "    :return: 注意力掩码\n",
    "    \"\"\"\n",
    "    # 设定subsequent_mask矩阵的shape\n",
    "    attn_shape = (1, size, size)\n",
    "    # 生成一个右上角(不含主对角线)为全1，左下角(含主对角线)为全0的subsequent_mask矩阵\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    # 返回一个右上角(不含主对角线)为全False，左下角(含主对角线)为全True的subsequent_mask矩阵\n",
    "    return torch.from_numpy(subsequent_mask) == 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97df4f92",
   "metadata": {},
   "source": [
    "## 5 词嵌入类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcaf8597",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    # 词嵌入层\n",
    "    def __init__(self, d_model, vocab):\n",
    "        \"\"\"\n",
    "        词嵌入层初始化\n",
    "        :param d_model: 词嵌入维度\n",
    "        :param vocab: 词表大小\n",
    "        \"\"\"\n",
    "        super(Embedding, self).__init__()\n",
    "        # Embedding层\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        # Embedding维数\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 返回x的词向量（需要乘以math.sqrt(d_model)）\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fccb95",
   "metadata": {},
   "source": [
    "## 6 位置编码器类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0c071d3",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    # 位置编码器层\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000, device='cuda'):\n",
    "        \"\"\"\n",
    "        位置编码器层初始化\n",
    "        :param d_model: 词嵌入维度\n",
    "        :param dropout: dropout比例\n",
    "        :param max_len: 序列最大长度\n",
    "        :param device: 训练设备\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        # 位置编码矩阵，维度[max_len, embedding_dim]\n",
    "        pe = torch.zeros(max_len, d_model, device=device)\n",
    "        # 单词位置\n",
    "        position = torch.arange(0.0, max_len, device=device)\n",
    "        position.unsqueeze_(1)\n",
    "        # 使用exp和log实现幂运算\n",
    "        div_term = torch.exp(torch.arange(0.0, d_model, 2, device=device) * (- math.log(1e4) / d_model))\n",
    "        div_term.unsqueeze_(0)\n",
    "        # 计算单词位置沿词向量维度的纹理值\n",
    "        pe[:, 0:: 2] = torch.sin(torch.mm(position, div_term))\n",
    "        pe[:, 1:: 2] = torch.cos(torch.mm(position, div_term))\n",
    "        # 增加批次维度，[1, max_len, embedding_dim]\n",
    "        pe.unsqueeze_(0)\n",
    "        # 将位置编码矩阵注册为buffer(不参加训练)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 将一个批次中语句所有词向量与位置编码相加\n",
    "        # 注意，位置编码不参与训练，因此设置requires_grad=False\n",
    "        x += Variable(self.pe[:, : x.size(1), :], requires_grad=False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82d6f3a",
   "metadata": {},
   "source": [
    "## 7 多头注意力机制类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d0349f0",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    # 多头注意力机制\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"\"\"\n",
    "        多头注意力机制初始化\n",
    "        :param h: 多头\n",
    "        :param d_model: 词嵌入维度\n",
    "        :param dropout: dropout比例\n",
    "        \"\"\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        # 确保整除\n",
    "        assert d_model % h == 0\n",
    "        # q、k、v向量维数\n",
    "        self.d_k = d_model // h\n",
    "        # 头的数量\n",
    "        self.h = h\n",
    "        # WQ、WK、WV矩阵及多头注意力拼接变换矩阵WO 4个线性层\n",
    "        self.linears = nn.ModuleList([copy.deepcopy(nn.Linear(d_model, d_model)) for _ in range(4)])\n",
    "        # 注意力机制函数\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        反向传播\n",
    "        :param query: q\n",
    "        :param key: k\n",
    "        :param value: v\n",
    "        :param mask: 掩码\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "        # 批次大小\n",
    "        nbatches = query.size(0)\n",
    "        # WQ、WK、WV分别对词向量线性变换，并将结果拆成h块\n",
    "        query, key, value = [\n",
    "            l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "            for l, x in zip(self.linears, (query, key, value))\n",
    "        ]\n",
    "        # 注意力加权\n",
    "        x, self.attn = self.attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "        # 多头注意力加权拼接\n",
    "        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)\n",
    "        # 对多头注意力加权拼接结果线性变换\n",
    "        return self.linears[-1](x)\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask=None, dropout=None):\n",
    "        \"\"\"\n",
    "        注意力加权\n",
    "        :param query: q\n",
    "        :param key: k\n",
    "        :param value: v\n",
    "        :param mask: 掩码矩阵\n",
    "        :param dropout: dropout比例\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # q、k、v向量长度为d_k\n",
    "        d_k = query.size(-1)\n",
    "        # 矩阵乘法实现q、k点积注意力，sqrt(d_k)归一化\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        # 注意力掩码机制\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        # 注意力矩阵softmax归一化\n",
    "        p_attn = F.softmax(scores, dim=-1)\n",
    "        # dropout\n",
    "        if dropout is not None:\n",
    "            p_attn = dropout(p_attn)\n",
    "        # 注意力对v加权\n",
    "        return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929c47ed",
   "metadata": {},
   "source": [
    "## 8 子层连接结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6de60f8a",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    # 子层连接结构 用于连接注意力机制以及前馈全连接网络\n",
    "    def __init__(self, d_model, dropout):\n",
    "        \"\"\"\n",
    "        子层连接结构初始化层\n",
    "        :param d_model: 词嵌入纬度\n",
    "        :param dropout: dropout比例\n",
    "        \"\"\"\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        # 规范化层\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        # dropout层\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        # 层归一化\n",
    "        x_ = self.norm(x)\n",
    "        x_ = sublayer(x_)\n",
    "        x_ = self.dropout(x_)\n",
    "        # 残差连接\n",
    "        return x + x_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf100c0",
   "metadata": {},
   "source": [
    "## 9 前馈全连接网络类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5a692cb",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    # 前馈全连接网络\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        前馈全连接网络初始化层\n",
    "        :param d_model: 词嵌入维度\n",
    "        :param d_ff: 中间隐层维度\n",
    "        :param dropout: dropout比例\n",
    "        \"\"\"\n",
    "        super(FeedForward, self).__init__()\n",
    "        # 全连接层\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        # Dropout层\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.w_1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.w_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b592b9ca",
   "metadata": {},
   "source": [
    "## 10 编码器类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acb56fa5",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    # 编码器\n",
    "    def __init__(self, h, d_model, d_ff=256, dropout=0.1):\n",
    "        \"\"\"\n",
    "        编码器层初始化\n",
    "        :param h: 头数\n",
    "        :param d_model: 词嵌入维度\n",
    "        :param d_ff: 中间隐层维度\n",
    "        :param dropout: dropout比例\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        # 多头注意力\n",
    "        self.self_attn = MultiHeadedAttention(h, d_model)\n",
    "        # 前馈全连接层\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        # 子层连接结构\n",
    "        self.sublayer1 = SublayerConnection(d_model, dropout)\n",
    "        self.sublayer2 = SublayerConnection(d_model, dropout)\n",
    "        # 规范化层\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # 将embedding层进行Multi head Attention\n",
    "        x = self.sublayer1(x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        # attn的结果直接作为下一层输入\n",
    "        return self.norm(self.sublayer2(x, self.feed_forward))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82943b33",
   "metadata": {},
   "source": [
    "## 11 解码器类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "facdc3b5",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, h, d_model, d_ff=256, dropout=0.1):\n",
    "        \"\"\"\n",
    "        解码器层\n",
    "        :param h: 头数\n",
    "        :param d_model: 词嵌入维度\n",
    "        :param d_ff: 中间隐层维度\n",
    "        :param dropout: dropout比例\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        self.size = d_model\n",
    "        # 自注意力机制\n",
    "        self.self_attn = MultiHeadedAttention(h, d_model)\n",
    "        # 上下文注意力机制\n",
    "        self.src_attn = MultiHeadedAttention(h, d_model)\n",
    "        # 前馈全连接子层\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        # 子层连接结构\n",
    "        self.sublayer1 = SublayerConnection(d_model, dropout)\n",
    "        self.sublayer2 = SublayerConnection(d_model, dropout)\n",
    "        self.sublayer3 = SublayerConnection(d_model, dropout)\n",
    "        # 规范化层\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        # memory为编码器输出隐表示\n",
    "        m = memory\n",
    "        # 自注意力机制，q、k、v均来自解码器隐表示\n",
    "        x = self.sublayer1(x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        # 上下文注意力机制：q为来自解码器隐表示，而k、v为编码器隐表示\n",
    "        x = self.sublayer2(x, lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        return self.norm(self.sublayer3(x, self.feed_forward))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8215038",
   "metadata": {},
   "source": [
    "## 12 生成器类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c09d0aac",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    #  生成器层\n",
    "    def __init__(self, d_model, vocab):\n",
    "        \"\"\"\n",
    "        生成器层初始化\n",
    "        :param d_model:\n",
    "        :param vocab:\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "        # decode后的结果，先进入一个全连接层变为词典大小的向量\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 然后再进行log_softmax操作(在softmax结果上再做多一次log运算)\n",
    "        return F.log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d318d0eb",
   "metadata": {},
   "source": [
    "## 13 transformer框架"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00026630",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    # Transformer层\n",
    "    def __init__(self, tokenizer, h=8, d_model=256, E_N=2, D_N=2, device='cuda'):\n",
    "        \"\"\"\n",
    "        transformer层初始化\n",
    "        :param h: 头数\n",
    "        :param d_model: 词嵌入纬度\n",
    "        :param tokenizer:\n",
    "        :param E_N:\n",
    "        :param D_N:\n",
    "        :param device:\n",
    "        \"\"\"\n",
    "        super(Transformer, self).__init__()\n",
    "        # 编码器\n",
    "        self.encoder = nn.ModuleList([Encoder(h, d_model) for _ in range(E_N)])\n",
    "        # 解码器\n",
    "        self.decoder = nn.ModuleList([Decoder(h, d_model) for _ in range(D_N)])\n",
    "        # 词嵌入层\n",
    "        self.src_embed = Embedding(d_model, tokenizer.get_vocab_size())\n",
    "        self.tgt_embed = Embedding(d_model, tokenizer.get_vocab_size())\n",
    "        # 位置编码器层\n",
    "        self.src_pos = PositionalEncoding(d_model, device=device)\n",
    "        self.tgt_pos = PositionalEncoding(d_model, device=device)\n",
    "        # 生成器层\n",
    "        self.generator = Generator(d_model, tokenizer.get_vocab_size())\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        \"\"\"\n",
    "        编码\n",
    "        :param src: 源数据\n",
    "        :param src_mask: 源数据掩码\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        \n",
    "        # 词嵌入\n",
    "        src = self.src_embed(src)\n",
    "        # 位置编码\n",
    "        src = self.src_pos(src)\n",
    "        # 编码\n",
    "        for i in self.encoder:\n",
    "            src = i(src, src_mask)\n",
    "        return src\n",
    "\n",
    "    def decode(self, memory, tgt, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "        解码\n",
    "        :param memory: 编码器输出\n",
    "        :param tgt: 目标数据输入\n",
    "        :param src_mask: 源数据掩码\n",
    "        :param tgt_mask: 目标数据掩码\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        #  词嵌入\n",
    "        tgt = self.tgt_embed(tgt)\n",
    "        #  位置编码\n",
    "        tgt = self.tgt_pos(tgt)\n",
    "        # 解码\n",
    "        for i in self.decoder:\n",
    "            tgt = i(tgt, memory, src_mask, tgt_mask)\n",
    "        return tgt\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "        反向传播\n",
    "        :param src: 源数据\n",
    "        :param tgt: 目标数据\n",
    "        :param src_mask: 源数据掩码\n",
    "        :param tgt_mask: 目标数据掩码\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # encoder的结果作为decoder的memory参数传入，进行decode\n",
    "        return self.decode(self.encode(src, src_mask), tgt, src_mask, tgt_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf8b6f2",
   "metadata": {},
   "source": [
    "## 14 标签平滑类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b02006b2",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    # 标签平滑\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        \"\"\"\n",
    "        初始化\n",
    "        :param size: 目标数据词表大小\n",
    "        :param padding_idx: 目标数据填充字符的索引\n",
    "        :param smoothing: 做平滑的值，为0即不进行平滑\n",
    "        \"\"\"\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        # KL散度，通常用于测量两个概率分布之间的差异\n",
    "        self.criterion = nn.KLDivLoss(reduction='sum')\n",
    "        # 目标数据填充字符的索引\n",
    "        self.padding_idx = padding_idx\n",
    "        # 置信度\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        # 平滑值\n",
    "        self.smoothing = smoothing\n",
    "        # 词表大小\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        \"\"\"\n",
    "        反向传播\n",
    "        :param x: 预测值\n",
    "        :param target: 目标值\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # 判断输出值的第二维传长度是否等于输出词表的大小，这里x的shape为 （batch*seqlength,x.shape(-1)）\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        # 标签平滑填充\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        # 这里的操作是将真实值的位置进行替换,替换成置信度\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        # 将填充的位置的值设置为0\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        # 生成填充部分的掩码\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        self.true_dist = true_dist\n",
    "        # 返回KL散度\n",
    "        return self.criterion(x, Variable(true_dist, requires_grad=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5ab2a5",
   "metadata": {},
   "source": [
    "## 15 损失计算类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c0f66c5",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "class SimpleLossCompute:\n",
    "    # 计算损失和进行参数反向传播\n",
    "    def __init__(self, generator, criterion, opt=None):\n",
    "        \"\"\"\n",
    "        初始化\n",
    "        :param generator: 生成器，transformer模块中的最后一层，这里将其单独拿出来而不直接放进transformer中的原因是：\n",
    "            预测数据的是时候，我们需要利用之前的结果，但是我们只去最后一个作为本次输出，那么在进行输出时，只对最后一个进行输出，单独拿出来进行输出的线性变换，更灵活\n",
    "        :param criterion: 标签平滑的类\n",
    "        :param opt: 经wormup后的optimizer\n",
    "        \"\"\"\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "        self.opt = opt\n",
    "\n",
    "    def __call__(self, x, y, norm):\n",
    "        \"\"\"\n",
    "        类做函数调用\n",
    "        :param x: 经transformer解码后的结果\n",
    "        :param y: 目标值\n",
    "        :param norm: 本次数据有效的字符数，即，除去padding后的字符数\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # 进行输出\n",
    "        x = self.generator(x)\n",
    "        # 得到KL散度\n",
    "        loss = self.criterion(x.contiguous().view(-1, x.size(-1)),\n",
    "                              y.contiguous().view(-1)) / norm\n",
    "        # 反向椽笔\n",
    "        loss.backward()\n",
    "        if self.opt is not None:\n",
    "            # 参数更新\n",
    "            self.opt.step()\n",
    "            # 优化器梯度置0\n",
    "            self.opt.optimizer.zero_grad()\n",
    "        # 返回损失\n",
    "        return loss.data.item() * norm.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea106b5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d01a5bc1",
   "metadata": {},
   "source": [
    "## 16 Warmup-学习率更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c275e7e0",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "class NoamOpt:\n",
    "    # warmup\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        \"\"\"\n",
    "        初始化\n",
    "        :param model_size: 词嵌入维度\n",
    "        :param factor:\n",
    "        :param warmup:\n",
    "        :param optimizer:\n",
    "        \"\"\"\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "\n",
    "    def step(self):\n",
    "        # 学习率更新\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "    def rate(self, step=None):\n",
    "        # 学习率更新函数\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * (self.model_size ** (-0.5) * min(step ** (-0.5), step * self.warmup ** (-1.5)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04bf5a5",
   "metadata": {},
   "source": [
    "## 评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a9fb0b3",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def is_english_sentence(sentence):\n",
    "    # 使用正则表达式检查句子中是否包含英文字母\n",
    "    english_pattern = re.compile(r'[a-zA-Z]')\n",
    "    match = english_pattern.search(sentence)\n",
    "    # True 表示这是英文句子\n",
    "    if match: \n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "# 这个smooth防止句子长度小于4而出现报错\n",
    "smooth = SmoothingFunction().method1\n",
    "def compute_bleu4(tokenizer, random_integers, model, device):\n",
    "    \"\"\"\n",
    "    计算BLEU4\n",
    "    :param tokenizer: tokenizer\n",
    "    :param random_integers: 这个是随机选择的测试集数据的编号\n",
    "    :param model: 模型\n",
    "    :param device: 设备\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # m1,m2存放英文的原数据与模型输出数据\n",
    "    m1, m2 = [], []\n",
    "    # m3,m4存放英文的原数据与模型输出数据\n",
    "    m3, m4 = [], []\n",
    "    model.eval()\n",
    "    # 存放测试数据\n",
    "    da = []\n",
    "    # 将随机选择的测试集数据编号添加到da中\n",
    "    for i in random_integers:\n",
    "        da.append(tokenizer.test[i])\n",
    "    # 对da中的数据进行编码\n",
    "    labels, x, _ = tokenizer.encode_all(da)\n",
    "    with torch.no_grad():\n",
    "        # 预测\n",
    "        y = predict(x, model, tokenizer, device)\n",
    "    # 这个p用于记录y的索引\n",
    "    p = 0\n",
    "    # 用于保存有效的索引\n",
    "    itg = []\n",
    "    # 这里我限制输入数据全部有效，如果有无效的数据，直接放弃本次计算\n",
    "    if len(y) != 10:\n",
    "        return 0\n",
    "    for i in labels:\n",
    "        # 取出有效的索引\n",
    "        itg.append(random_integers[i])\n",
    "    # 将真实数据与预测数据分别放到m1,m2,m3,m4中\n",
    "    for i in itg:\n",
    "        if is_english_sentence(tokenizer.test[i][1]):\n",
    "            m1.append(tokenizer.test[i][1])\n",
    "            m2.append([y[p]])\n",
    "        else:\n",
    "            m3.append(list(jieba.cut(tokenizer.test[i][1])))\n",
    "            m4.append([list(jieba.cut(y[p]))])\n",
    "        p += 1\n",
    "    smooth = SmoothingFunction().method1\n",
    "    # 计算英文的bleu4\n",
    "    b1 = [sacrebleu.sentence_bleu(candidate, refs).score for candidate, refs in zip(m1, m2)]\n",
    "    # 计算中文的bleu4\n",
    "    for i in range(len(m4)):\n",
    "        b2 = sentence_bleu(m4[i], m3[i], weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smooth) * 100\n",
    "        b1.append(b2)\n",
    "#     print(b1)\n",
    "#     print(sum(b1)/len(b1))\n",
    "    return sum(b1)/len(b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb47ba2",
   "metadata": {},
   "source": [
    "## 17 训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c384beae",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "英文 0.0\n",
      "英文 0.014628651677174915\n",
      "英文 0.02925730335434983\n",
      "英文 0.043885955031524745\n",
      "英文 0.05851460670869966\n",
      "英文 0.07314325838587457\n",
      "英文 0.08777191006304949\n",
      "英文 0.1024005617402244\n",
      "英文 0.11702921341739932\n",
      "英文 0.13165786509457422\n",
      "英文 0.14628651677174914\n",
      "英文 0.16091516844892406\n",
      "英文 0.17554382012609898\n",
      "英文 0.1901724718032739\n",
      "英文 0.2048011234804488\n",
      "英文 0.2194297751576237\n",
      "英文 0.23405842683479863\n",
      "英文 0.24868707851197355\n",
      "英文 0.26331573018914844\n",
      "英文 0.27794438186632336\n",
      "英文 0.2925730335434983\n",
      "英文 0.3072016852206732\n",
      "英文 0.3218303368978481\n",
      "英文 0.33645898857502304\n",
      "英文 0.35108764025219796\n",
      "英文 0.3657162919293729\n",
      "英文 0.3803449436065478\n",
      "英文 0.3949735952837227\n",
      "英文 0.4096022469608976\n",
      "英文 0.4242308986380725\n",
      "英文 0.4388595503152474\n",
      "英文 0.45348820199242235\n",
      "英文 0.46811685366959727\n",
      "英文 0.4827455053467722\n",
      "英文 0.4973741570239471\n",
      "英文 0.512002808701122\n",
      "英文 0.5266314603782969\n",
      "英文 0.5412601120554719\n",
      "英文 0.5558887637326467\n",
      "英文 0.5705174154098217\n",
      "英文 0.5851460670869966\n",
      "英文 0.5997747187641715\n",
      "英文 0.6144033704413464\n",
      "英文 0.6290320221185214\n",
      "英文 0.6436606737956962\n",
      "英文 0.6582893254728711\n",
      "英文 0.6729179771500461\n",
      "英文 0.687546628827221\n",
      "英文 0.7021752805043959\n",
      "英文 0.7168039321815708\n",
      "英文 0.7314325838587458\n",
      "英文 0.7460612355359206\n",
      "英文 0.7606898872130956\n",
      "英文 0.7753185388902705\n",
      "英文 0.7899471905674454\n",
      "英文 0.8045758422446203\n",
      "英文 0.8192044939217952\n",
      "英文 0.8338331455989701\n",
      "英文 0.848461797276145\n",
      "英文 0.86309044895332\n",
      "英文 0.8777191006304949\n",
      "英文 0.8923477523076698\n",
      "英文 0.9069764039848447\n",
      "英文 0.9216050556620197\n",
      "英文 0.9362337073391945\n",
      "英文 0.9508623590163695\n",
      "英文 0.9654910106935444\n",
      "英文 0.9801196623707192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "英文 0.9947483140478942\n",
      "中文 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.914 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中文 0.014628651677174915\n",
      "中文 0.02925730335434983\n",
      "中文 0.043885955031524745\n",
      "中文 0.05851460670869966\n",
      "中文 0.07314325838587457\n",
      "中文 0.08777191006304949\n",
      "中文 0.1024005617402244\n",
      "中文 0.11702921341739932\n",
      "中文 0.13165786509457422\n",
      "中文 0.14628651677174914\n",
      "中文 0.16091516844892406\n",
      "中文 0.17554382012609898\n",
      "中文 0.1901724718032739\n",
      "中文 0.2048011234804488\n",
      "中文 0.2194297751576237\n",
      "中文 0.23405842683479863\n",
      "中文 0.24868707851197355\n",
      "中文 0.26331573018914844\n",
      "中文 0.27794438186632336\n",
      "中文 0.2925730335434983\n",
      "中文 0.3072016852206732\n",
      "中文 0.3218303368978481\n",
      "中文 0.33645898857502304\n",
      "中文 0.35108764025219796\n",
      "中文 0.3657162919293729\n",
      "中文 0.3803449436065478\n",
      "中文 0.3949735952837227\n",
      "中文 0.4096022469608976\n",
      "中文 0.4242308986380725\n",
      "中文 0.4388595503152474\n",
      "中文 0.45348820199242235\n",
      "中文 0.46811685366959727\n",
      "中文 0.4827455053467722\n",
      "中文 0.4973741570239471\n",
      "中文 0.512002808701122\n",
      "中文 0.5266314603782969\n",
      "中文 0.5412601120554719\n",
      "中文 0.5558887637326467\n",
      "中文 0.5705174154098217\n",
      "中文 0.5851460670869966\n",
      "中文 0.5997747187641715\n",
      "中文 0.6144033704413464\n",
      "中文 0.6290320221185214\n",
      "中文 0.6436606737956962\n",
      "中文 0.6582893254728711\n",
      "中文 0.6729179771500461\n",
      "中文 0.687546628827221\n",
      "中文 0.7021752805043959\n",
      "中文 0.7168039321815708\n",
      "中文 0.7314325838587458\n",
      "中文 0.7460612355359206\n",
      "中文 0.7606898872130956\n",
      "中文 0.7753185388902705\n",
      "中文 0.7899471905674454\n",
      "中文 0.8045758422446203\n",
      "中文 0.8192044939217952\n",
      "中文 0.8338331455989701\n",
      "中文 0.848461797276145\n",
      "中文 0.86309044895332\n",
      "中文 0.8777191006304949\n",
      "中文 0.8923477523076698\n",
      "中文 0.9069764039848447\n",
      "中文 0.9216050556620197\n",
      "中文 0.9362337073391945\n",
      "中文 0.9508623590163695\n",
      "中文 0.9654910106935444\n",
      "中文 0.9801196623707192\n",
      "中文 0.9947483140478942\n"
     ]
    }
   ],
   "source": [
    "en_path = r'./src.txt'\n",
    "ch_path = r'./tgt.txt'\n",
    "tokenizer = Tokenizer(en_path, ch_path, count_min=3)\n",
    "# 训练\n",
    "def train(): \n",
    "    device = 'cuda'\n",
    "    model = Transformer(tokenizer, device=device)\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            # 这里初始化采用的是nn.init.xavier_uniform\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    model = model.to(device)\n",
    "    criteria = LabelSmoothing(tokenizer.get_vocab_size(), tokenizer.word_2_index['<pad>'])\n",
    "    optimizer = NoamOpt(256, 1, 2000,\n",
    "                        torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
    "    lossF = SimpleLossCompute(model.generator, criteria, optimizer)\n",
    "    epochs = 100\n",
    "    model.train()\n",
    "    loss_all = []\n",
    "    print('词表大小', tokenizer.get_vocab_size())\n",
    "    t = time.time()\n",
    "    data_loader = tokenizer.get_dataloader(tokenizer.data_)\n",
    "    random_integers = random.sample(range(len(tokenizer.test)-10), 6)  # 随机选100个句子\n",
    "    batchs=[]\n",
    "    for index, data in enumerate(data_loader):\n",
    "        src, tgt = data\n",
    "        # 处理一个batch\n",
    "        batch = Batch(src, tgt, tokenizer=tokenizer, device=device)\n",
    "        batchs.append(batch)\n",
    "    for epoch in range(epochs):\n",
    "        p=0\n",
    "        for batch in batchs:\n",
    "            out = model(batch.src, batch.trg, batch.src_mask, batch.trg_mask)\n",
    "            out = lossF(out, batch.trg_y, batch.ntokens)\n",
    "            if (p+1) % 1000 == 0:\n",
    "                model.eval()\n",
    "#                 compute_bleu4(tokenizer, random_integers, model, device)\n",
    "                print('epoch', epoch, 'loss', float(out / batch.ntokens))\n",
    "                model.train()\n",
    "                print('time', time.time() - t)\n",
    "                if float(out / batch.ntokens)<2.2:\n",
    "                    random_integers = random.sample(range(len(tokenizer.test)), 100)\n",
    "                    nu=compute_bleu4(tokenizer, random_integers, model, device)\n",
    "                    if nu > 17:\n",
    "                        torch.save(model.state_dict(), f'./save/translation_{epoch}_{p}.pt')\n",
    "                        break\n",
    "                    if nu > 14:\n",
    "                        torch.save(model.state_dict(), f'./save/translation_{epoch}_{p}.pt')\n",
    "\n",
    "            if p%100==0:\n",
    "                print(p/1000)\n",
    "            p+=1\n",
    "        \n",
    "        loss_all.append(float(out / batch.ntokens))\n",
    "        \n",
    "        \n",
    "\n",
    "    with open('loss.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write(str(loss_all))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59aecc9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "298abc2a",
   "metadata": {},
   "source": [
    "## 18 预测函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af7ad45d",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import words\n",
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    \"\"\"\n",
    "    传入一个训练好的模型，对指定数据进行预测\n",
    "    \"\"\"\n",
    "    # 先用encoder进行encode\n",
    "    memory = model.encode(src, src_mask)\n",
    "    # 初始化预测内容为1×1的tensor，填入开始符('BOS')的id，并将type设置为输入数据类型(LongTensor)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
    "    # 遍历输出的长度下标\n",
    "    for i in range(max_len - 1):\n",
    "        # decode得到隐层表示\n",
    "        out = model.decode(memory,\n",
    "                           Variable(ys),\n",
    "                           src_mask,\n",
    "                           Variable(subsequent_mask(ys.size(1)).type_as(src.data)))\n",
    "        # 将隐藏表示转为对词典各词的log_softmax概率分布表示\n",
    "        prob = model.generator(out[:, i])\n",
    "        # 获取当前位置最大概率的预测词id\n",
    "        _, next_word = torch.max(prob, dim=-1)\n",
    "        next_word = next_word.data[0]\n",
    "        # 将当前位置预测的字符id与之前的预测内容拼接起来\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
    "    return ys\n",
    "\n",
    "\n",
    "def predict(data, model, tokenizer, device='cuda'):\n",
    "    \"\"\"\n",
    "    在data上用训练好的模型进行预测，打印模型翻译结果\n",
    "    \"\"\"\n",
    "    # 梯度清零\n",
    "    with torch.no_grad():\n",
    "        # 在data的英文数据长度上遍历下标\n",
    "        data1=[]\n",
    "        for i in range(len(data)):\n",
    "            # 打印待翻译的英文语句\n",
    "\n",
    "            # 将当前以单词id表示的英文语句数据转为tensor，并放如DEVICE中\n",
    "            src = torch.from_numpy(np.array(data[i])).long().to(device)\n",
    "            # 增加一维\n",
    "            src = src.unsqueeze(0)\n",
    "            # 设置attention mask\n",
    "            src_mask = (src != tokenizer.word_2_index['<pad>']).unsqueeze(-2)\n",
    "            # 用训练好的模型进行decode预测\n",
    "            out = greedy_decode(model, src, src_mask, max_len=100, start_symbol=tokenizer.word_2_index['<bos>'])\n",
    "            # 初始化一个用于存放模型翻译结果语句单词的列表\n",
    "            translation = []\n",
    "            # 遍历翻译输出字符的下标（注意：开始符\"BOS\"的索引0不遍历）\n",
    "            for j in range(1, out.size(1)):\n",
    "                # 获取当前下标的输出字符\n",
    "                sym = tokenizer.index_2_word[out[0, j].item()]\n",
    "                # 如果输出字符不为'EOS'终止符，则添加到当前语句的翻译结果列表\n",
    "                if sym != '<eos>':\n",
    "                    translation.append(sym)\n",
    "                # 否则终止遍历\n",
    "                else:\n",
    "                    break\n",
    "            # 打印模型翻译输出的中文语句结果\n",
    "            if len(translation)>0:\n",
    "                if translation[0].lower() in words.words():\n",
    "                    data1.append(TreebankWordDetokenizer().detokenize(translation))\n",
    "                else:\n",
    "                    data1.append(\"\".join(translation))\n",
    "        return data1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54989ea",
   "metadata": {},
   "source": [
    "## 19 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2496758e",
   "metadata": {
    "metadata": {},
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词表大小 21270\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 0 loss 4.645392894744873\n",
      "time 95.02873969078064\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 0 loss 4.369634628295898\n",
      "time 154.63299441337585\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 0 loss 4.239884376525879\n",
      "time 213.8566358089447\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 1 loss 4.036822319030762\n",
      "time 295.89451789855957\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 1 loss 3.898416042327881\n",
      "time 352.8930242061615\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 1 loss 3.843503713607788\n",
      "time 410.43497490882874\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 2 loss 3.770007848739624\n",
      "time 492.03778314590454\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 2 loss 3.6062240600585938\n",
      "time 550.7655227184296\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 2 loss 3.7219386100769043\n",
      "time 608.8678636550903\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 3 loss 3.6204257011413574\n",
      "time 689.5660083293915\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 3 loss 3.5241522789001465\n",
      "time 747.7584826946259\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 3 loss 3.5350539684295654\n",
      "time 806.3219335079193\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 4 loss 3.498889446258545\n",
      "time 887.6820392608643\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 4 loss 3.403928756713867\n",
      "time 946.4188294410706\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 4 loss 3.532268762588501\n",
      "time 1005.2287693023682\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 5 loss 3.353914737701416\n",
      "time 1086.1028578281403\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 5 loss 3.4067816734313965\n",
      "time 1143.8319280147552\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 5 loss 3.3875033855438232\n",
      "time 1201.6534478664398\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 6 loss 3.2878215312957764\n",
      "time 1282.476126909256\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 6 loss 3.3414158821105957\n",
      "time 1340.6219007968903\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 6 loss 3.3370776176452637\n",
      "time 1399.0284204483032\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 7 loss 3.209543228149414\n",
      "time 1480.6489231586456\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 7 loss 3.292013168334961\n",
      "time 1540.0193545818329\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 7 loss 3.310739517211914\n",
      "time 1598.2050862312317\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 8 loss 3.190268039703369\n",
      "time 1676.054449081421\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 8 loss 3.158689260482788\n",
      "time 1715.0066976547241\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 8 loss 3.2245166301727295\n",
      "time 1754.1094686985016\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 9 loss 3.1055493354797363\n",
      "time 1808.4378325939178\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 9 loss 3.106520891189575\n",
      "time 1847.523525238037\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 9 loss 3.1106209754943848\n",
      "time 1886.7064368724823\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 10 loss 3.059631586074829\n",
      "time 1941.3001503944397\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 10 loss 3.084911346435547\n",
      "time 1980.678477048874\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 10 loss 3.135129690170288\n",
      "time 2019.8277215957642\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 11 loss 2.9806840419769287\n",
      "time 2074.717990398407\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 11 loss 2.971853256225586\n",
      "time 2114.0907933712006\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 11 loss 3.020477771759033\n",
      "time 2153.3052463531494\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 12 loss 2.925100803375244\n",
      "time 2207.4289379119873\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 12 loss 2.949470281600952\n",
      "time 2246.2880799770355\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 12 loss 2.9499993324279785\n",
      "time 2285.291689157486\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 13 loss 2.873486042022705\n",
      "time 2339.556795835495\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 13 loss 2.9079344272613525\n",
      "time 2378.453924894333\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 13 loss 3.009436845779419\n",
      "time 2417.3479545116425\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 14 loss 2.8219804763793945\n",
      "time 2471.893410682678\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 14 loss 2.8664634227752686\n",
      "time 2511.365354537964\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 14 loss 2.9836585521698\n",
      "time 2550.791203737259\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 15 loss 2.802262783050537\n",
      "time 2605.5286519527435\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 15 loss 2.896949052810669\n",
      "time 2644.8040096759796\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 15 loss 2.859278917312622\n",
      "time 2686.43493103981\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 16 loss 2.789762258529663\n",
      "time 2766.8229229450226\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 16 loss 2.811183452606201\n",
      "time 2825.569507598877\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 16 loss 2.8772623538970947\n",
      "time 2884.9259979724884\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 17 loss 2.743061065673828\n",
      "time 2967.1680862903595\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 17 loss 2.831577777862549\n",
      "time 3026.3711717128754\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 17 loss 2.9199233055114746\n",
      "time 3085.3828291893005\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 18 loss 2.744910955429077\n",
      "time 3168.093543767929\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 18 loss 2.7557644844055176\n",
      "time 3227.356009721756\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 18 loss 2.791411876678467\n",
      "time 3286.7109603881836\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 19 loss 2.719581127166748\n",
      "time 3369.7179112434387\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 19 loss 2.7266759872436523\n",
      "time 3428.965375185013\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 19 loss 2.8136813640594482\n",
      "time 3488.218777179718\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 20 loss 2.631549119949341\n",
      "time 3571.2214834690094\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 20 loss 2.7468998432159424\n",
      "time 3631.090355157852\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 20 loss 2.803391695022583\n",
      "time 3690.9955418109894\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 21 loss 2.7148277759552\n",
      "time 3774.082467317581\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 21 loss 2.6349406242370605\n",
      "time 3834.0663974285126\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 21 loss 2.715745210647583\n",
      "time 3893.979392051697\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 22 loss 2.6667706966400146\n",
      "time 3977.0237321853638\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 22 loss 2.7357568740844727\n",
      "time 4035.6902148723602\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 22 loss 2.769171714782715\n",
      "time 4094.677382707596\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 23 loss 2.5832901000976562\n",
      "time 4176.323677778244\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 23 loss 2.6743688583374023\n",
      "time 4235.140566825867\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 23 loss 2.7504446506500244\n",
      "time 4294.027218341827\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 24 loss 2.6124961376190186\n",
      "time 4376.50515794754\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 24 loss 2.731116771697998\n",
      "time 4435.944172143936\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 24 loss 2.748058557510376\n",
      "time 4495.575363636017\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 25 loss 2.6082851886749268\n",
      "time 4578.361419916153\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 25 loss 2.6186583042144775\n",
      "time 4637.860216856003\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 25 loss 2.7897450923919678\n",
      "time 4685.739234209061\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 26 loss 2.666137933731079\n",
      "time 4739.889546632767\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 26 loss 2.6891391277313232\n",
      "time 4778.944356203079\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 26 loss 2.7977077960968018\n",
      "time 4818.181436538696\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 27 loss 2.7279090881347656\n",
      "time 4872.731156826019\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 27 loss 2.6588356494903564\n",
      "time 4911.927625656128\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 27 loss 2.754394054412842\n",
      "time 4951.143385410309\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 28 loss 2.7022593021392822\n",
      "time 5005.3773674964905\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 28 loss 2.6531527042388916\n",
      "time 5044.608701944351\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 28 loss 2.77068829536438\n",
      "time 5083.943231582642\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 29 loss 2.671534299850464\n",
      "time 5138.502653598785\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 29 loss 2.679119348526001\n",
      "time 5177.867560386658\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 29 loss 2.8704640865325928\n",
      "time 5217.011969804764\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 30 loss 2.669584274291992\n",
      "time 5271.555433034897\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 30 loss 2.746673345565796\n",
      "time 5310.930508613586\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 30 loss 2.8338470458984375\n",
      "time 5350.298012018204\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 31 loss 2.697049379348755\n",
      "time 5405.068866729736\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 31 loss 2.7043139934539795\n",
      "time 5444.492854595184\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 31 loss 2.8592076301574707\n",
      "time 5484.063483715057\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 32 loss 2.7860844135284424\n",
      "time 5538.986571073532\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 32 loss 2.847930431365967\n",
      "time 5578.522380590439\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 32 loss 2.8852574825286865\n",
      "time 5617.97366309166\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 33 loss 2.7168025970458984\n",
      "time 5672.8234622478485\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 33 loss 2.7465052604675293\n",
      "time 5712.141940116882\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 33 loss 2.8968920707702637\n",
      "time 5751.351631164551\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 34 loss 2.753361940383911\n",
      "time 5805.861900806427\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 34 loss 2.794853925704956\n",
      "time 5845.072940587997\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 34 loss 2.8453996181488037\n",
      "time 5884.231732368469\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 35 loss 2.752840995788574\n",
      "time 5938.55926823616\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 35 loss 2.8363869190216064\n",
      "time 5977.897508859634\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 35 loss 2.788339376449585\n",
      "time 6016.99999165535\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 36 loss 2.7291016578674316\n",
      "time 6071.737275362015\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 36 loss 2.7416605949401855\n",
      "time 6110.827316045761\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 36 loss 2.8846147060394287\n",
      "time 6149.775471687317\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 37 loss 2.711528778076172\n",
      "time 6204.178452253342\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 37 loss 2.6987226009368896\n",
      "time 6244.885016202927\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 37 loss 2.856757402420044\n",
      "time 6301.57389497757\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 38 loss 2.7380003929138184\n",
      "time 6382.678001880646\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 38 loss 2.668229103088379\n",
      "time 6441.877652645111\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 38 loss 2.8404674530029297\n",
      "time 6501.040783882141\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 39 loss 2.771916389465332\n",
      "time 6583.471195459366\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 39 loss 2.783500909805298\n",
      "time 6642.611953496933\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 39 loss 2.8060073852539062\n",
      "time 6703.961166858673\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 40 loss 2.73858380317688\n",
      "time 6786.818695783615\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 40 loss 2.7111546993255615\n",
      "time 6846.120208501816\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 40 loss 2.8094305992126465\n",
      "time 6905.1906406879425\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 41 loss 2.8012583255767822\n",
      "time 6987.488379240036\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 41 loss 2.6936254501342773\n",
      "time 7046.431780099869\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 41 loss 2.7697646617889404\n",
      "time 7105.451603651047\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 42 loss 2.643439292907715\n",
      "time 7188.468387365341\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 42 loss 2.7136611938476562\n",
      "time 7248.331529855728\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 42 loss 2.7865710258483887\n",
      "time 7307.875806808472\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 43 loss 2.6719555854797363\n",
      "time 7391.154590606689\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 43 loss 2.650932788848877\n",
      "time 7450.447002410889\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 43 loss 2.7538816928863525\n",
      "time 7510.019437789917\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 44 loss 2.649390697479248\n",
      "time 7592.6502594947815\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 44 loss 2.698105812072754\n",
      "time 7652.294571638107\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 44 loss 2.8163790702819824\n",
      "time 7712.149742126465\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 45 loss 2.6559512615203857\n",
      "time 7795.5324375629425\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 45 loss 2.6802823543548584\n",
      "time 7855.366933345795\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 45 loss 2.712698221206665\n",
      "time 7915.560797214508\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 46 loss 2.6138432025909424\n",
      "time 7999.061023950577\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 46 loss 2.654860734939575\n",
      "time 8058.598926305771\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 46 loss 2.761061906814575\n",
      "time 8117.821177959442\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 47 loss 2.648956775665283\n",
      "time 8200.536015033722\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 47 loss 2.6179070472717285\n",
      "time 8259.434912443161\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 47 loss 2.746401071548462\n",
      "time 8318.738404750824\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 48 loss 2.5918967723846436\n",
      "time 8400.814678907394\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 48 loss 2.618896961212158\n",
      "time 8459.687366962433\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 48 loss 2.7681589126586914\n",
      "time 8518.77203297615\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 49 loss 2.6705477237701416\n",
      "time 8601.467281341553\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 49 loss 2.569788932800293\n",
      "time 8660.76811337471\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 49 loss 2.687274694442749\n",
      "time 8720.465019702911\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 50 loss 2.6212897300720215\n",
      "time 8803.903101444244\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 50 loss 2.6179442405700684\n",
      "time 8863.640006780624\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 50 loss 2.675302743911743\n",
      "time 8923.533224582672\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 51 loss 2.648691415786743\n",
      "time 9006.55330824852\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 51 loss 2.64383602142334\n",
      "time 9065.137751579285\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 51 loss 2.6589341163635254\n",
      "time 9124.205909729004\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 52 loss 2.693246603012085\n",
      "time 9206.266702890396\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 52 loss 2.5836503505706787\n",
      "time 9265.496171474457\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 52 loss 2.649073600769043\n",
      "time 9324.595348358154\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 53 loss 2.6071937084198\n",
      "time 9407.727924823761\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 53 loss 2.5148870944976807\n",
      "time 9467.32561135292\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 53 loss 2.651247024536133\n",
      "time 9526.771142721176\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 54 loss 2.6231746673583984\n",
      "time 9609.692754268646\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 54 loss 2.518829822540283\n",
      "time 9669.414437294006\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 54 loss 2.7167160511016846\n",
      "time 9729.027690172195\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 55 loss 2.573530912399292\n",
      "time 9812.434110164642\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 55 loss 2.524783134460449\n",
      "time 9872.312173604965\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 55 loss 2.6383771896362305\n",
      "time 9932.199340581894\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 56 loss 2.5612637996673584\n",
      "time 10014.365520954132\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 56 loss 2.5369274616241455\n",
      "time 10073.31141114235\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 56 loss 2.6557672023773193\n",
      "time 10132.497844219208\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 57 loss 2.602968454360962\n",
      "time 10214.746892213821\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 57 loss 2.518908977508545\n",
      "time 10274.108214616776\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 57 loss 2.6382508277893066\n",
      "time 10333.585315227509\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 58 loss 2.5429718494415283\n",
      "time 10416.492822647095\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 58 loss 2.552532434463501\n",
      "time 10475.671887159348\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 58 loss 2.572654962539673\n",
      "time 10535.235336303711\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 59 loss 2.4968278408050537\n",
      "time 10618.549460172653\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 59 loss 2.4616799354553223\n",
      "time 10678.605114221573\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 59 loss 2.485152244567871\n",
      "time 10738.438472032547\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 60 loss 2.618156671524048\n",
      "time 10821.888190984726\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 60 loss 2.4930381774902344\n",
      "time 10881.673050165176\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 60 loss 2.500001907348633\n",
      "time 10940.65793299675\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 61 loss 2.552389144897461\n",
      "time 11022.573635101318\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 61 loss 2.5399584770202637\n",
      "time 11081.695303201675\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 61 loss 2.4140055179595947\n",
      "time 11140.957158327103\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 62 loss 2.529550552368164\n",
      "time 11223.359619140625\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 62 loss 2.4436748027801514\n",
      "time 11283.155702829361\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 62 loss 2.486348867416382\n",
      "time 11342.947340726852\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 63 loss 2.480414390563965\n",
      "time 11426.041125535965\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 63 loss 2.380894422531128\n",
      "time 11485.516608715057\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 63 loss 2.4935226440429688\n",
      "time 11545.27736902237\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 64 loss 2.4863576889038086\n",
      "time 11628.698659181595\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 64 loss 2.398728132247925\n",
      "time 11688.784798145294\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 64 loss 2.491086006164551\n",
      "time 11748.656621456146\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 65 loss 2.531301498413086\n",
      "time 11831.212208747864\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 65 loss 2.470550060272217\n",
      "time 11889.979799032211\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 65 loss 2.542635202407837\n",
      "time 11948.903889656067\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 66 loss 2.4692208766937256\n",
      "time 12030.964061021805\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 66 loss 2.478651285171509\n",
      "time 12090.466391801834\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 66 loss 2.544584274291992\n",
      "time 12149.891094207764\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 67 loss 2.570918560028076\n",
      "time 12232.799696922302\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 67 loss 2.528635263442993\n",
      "time 12292.140897035599\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 67 loss 2.5726821422576904\n",
      "time 12351.805071353912\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 68 loss 2.548989772796631\n",
      "time 12434.472127914429\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 68 loss 2.520263195037842\n",
      "time 12493.947063684464\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 68 loss 2.5633440017700195\n",
      "time 12553.97999382019\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 69 loss 2.5502631664276123\n",
      "time 12637.56066775322\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 69 loss 2.437488079071045\n",
      "time 12697.18545460701\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 69 loss 2.5003905296325684\n",
      "time 12757.068356513977\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 70 loss 2.518519401550293\n",
      "time 12840.374227762222\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 70 loss 2.433647394180298\n",
      "time 12899.276268959045\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 70 loss 2.561042070388794\n",
      "time 12958.309356689453\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 71 loss 2.469839096069336\n",
      "time 13041.192988157272\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 71 loss 2.500082015991211\n",
      "time 13100.637902259827\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 71 loss 2.5602238178253174\n",
      "time 13160.02853512764\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 72 loss 2.471459150314331\n",
      "time 13242.513693094254\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 72 loss 2.470468521118164\n",
      "time 13301.734225034714\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 72 loss 2.521437406539917\n",
      "time 13360.912606477737\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 73 loss 2.4620304107666016\n",
      "time 13443.953809976578\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 73 loss 2.3989171981811523\n",
      "time 13503.423387527466\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 73 loss 2.5939695835113525\n",
      "time 13562.939213991165\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 74 loss 2.5463032722473145\n",
      "time 13646.75690984726\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 74 loss 2.4419355392456055\n",
      "time 13706.37388586998\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 74 loss 2.5629007816314697\n",
      "time 13766.088881254196\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 75 loss 2.4700000286102295\n",
      "time 13849.403816699982\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 75 loss 2.3891382217407227\n",
      "time 13909.193123579025\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 75 loss 2.5652341842651367\n",
      "time 13968.53803730011\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 76 loss 2.469055414199829\n",
      "time 14050.692281723022\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 76 loss 2.489931106567383\n",
      "time 14109.723055839539\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 76 loss 2.569246768951416\n",
      "time 14168.520786046982\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 77 loss 2.5039968490600586\n",
      "time 14250.827212572098\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 77 loss 2.458864688873291\n",
      "time 14309.983058214188\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 77 loss 2.5260002613067627\n",
      "time 14369.229382753372\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 78 loss 2.527397632598877\n",
      "time 14452.263136148453\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 78 loss 2.4545161724090576\n",
      "time 14511.729013681412\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 78 loss 2.519869804382324\n",
      "time 14571.419125318527\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 79 loss 2.4692912101745605\n",
      "time 14654.729855298996\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 79 loss 2.4502625465393066\n",
      "time 14714.768550157547\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 79 loss 2.581062078475952\n",
      "time 14774.597392559052\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 80 loss 2.5119268894195557\n",
      "time 14858.010292768478\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 80 loss 2.49187970161438\n",
      "time 14917.506448507309\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 80 loss 2.5083839893341064\n",
      "time 14976.600534915924\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 81 loss 2.4561214447021484\n",
      "time 15058.630870819092\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 81 loss 2.5672411918640137\n",
      "time 15117.857469558716\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 81 loss 2.5635011196136475\n",
      "time 15177.150623559952\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 82 loss 2.497027635574341\n",
      "time 15259.205345869064\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 82 loss 2.4222803115844727\n",
      "time 15318.69689321518\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 82 loss 2.6051714420318604\n",
      "time 15378.374831676483\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 83 loss 2.5317702293395996\n",
      "time 15461.510912656784\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 83 loss 2.4410512447357178\n",
      "time 15521.142522335052\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 83 loss 2.613058567047119\n",
      "time 15580.955387592316\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 84 loss 2.4106314182281494\n",
      "time 15664.264666080475\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 84 loss 2.386652946472168\n",
      "time 15724.059611558914\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 84 loss 2.620328903198242\n",
      "time 15783.933701992035\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 85 loss 2.3876864910125732\n",
      "time 15867.092277050018\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 85 loss 2.4514758586883545\n",
      "time 15926.031799554825\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 85 loss 2.4894192218780518\n",
      "time 15985.428879737854\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 86 loss 2.413709878921509\n",
      "time 16067.776100635529\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 86 loss 2.486649990081787\n",
      "time 16127.052640676498\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 86 loss 2.57098126411438\n",
      "time 16186.50920009613\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 87 loss 2.373256206512451\n",
      "time 16269.39342880249\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 87 loss 2.4696388244628906\n",
      "time 16328.897553443909\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 87 loss 2.522362470626831\n",
      "time 16388.401885032654\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 88 loss 2.403095245361328\n",
      "time 16471.21631026268\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 88 loss 2.3602702617645264\n",
      "time 16530.801507234573\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 88 loss 2.5032286643981934\n",
      "time 16590.796176195145\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 89 loss 2.447969675064087\n",
      "time 16674.249995470047\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 89 loss 2.4196345806121826\n",
      "time 16733.867081165314\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 89 loss 2.4632813930511475\n",
      "time 16793.67415046692\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 90 loss 2.4647789001464844\n",
      "time 16875.69195008278\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 90 loss 2.391791582107544\n",
      "time 16934.617451667786\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 90 loss 2.5296475887298584\n",
      "time 16994.038080453873\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 91 loss 2.4745101928710938\n",
      "time 17076.68443584442\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 91 loss 2.4696054458618164\n",
      "time 17136.1912252903\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 91 loss 2.5434041023254395\n",
      "time 17196.087508916855\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 92 loss 2.426555633544922\n",
      "time 17279.484333992004\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 92 loss 2.395235061645508\n",
      "time 17339.374705314636\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 92 loss 2.551978349685669\n",
      "time 17399.42603445053\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 93 loss 2.3742222785949707\n",
      "time 17482.301132678986\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 93 loss 2.4261322021484375\n",
      "time 17541.35548877716\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 93 loss 2.4886903762817383\n",
      "time 17600.553283691406\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 94 loss 2.4463448524475098\n",
      "time 17683.16665816307\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 94 loss 2.3533928394317627\n",
      "time 17742.50145316124\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 94 loss 2.414027452468872\n",
      "time 17801.720755815506\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 95 loss 2.313048839569092\n",
      "time 17884.72797894478\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 95 loss 2.319817304611206\n",
      "time 17944.61891770363\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 95 loss 2.545271635055542\n",
      "time 18004.425758123398\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 96 loss 2.3647613525390625\n",
      "time 18087.856313943863\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 96 loss 2.4053077697753906\n",
      "time 18147.49114537239\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 96 loss 2.479759931564331\n",
      "time 18207.487006664276\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 97 loss 2.421588897705078\n",
      "time 18290.46200990677\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 97 loss 2.3882195949554443\n",
      "time 18349.399970293045\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 97 loss 2.510345458984375\n",
      "time 18408.425886392593\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 98 loss 2.433915376663208\n",
      "time 18490.71953368187\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 98 loss 2.3149118423461914\n",
      "time 18549.934645175934\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 98 loss 2.5010368824005127\n",
      "time 18609.268736600876\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 99 loss 2.4051711559295654\n",
      "time 18692.01817369461\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 99 loss 2.305516004562378\n",
      "time 18751.87161540985\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 99 loss 2.536153554916382\n",
      "time 18811.664720773697\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1b54981e",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# %matplotlib qt\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a9a7b4cd",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3175960/720591538.py:8: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  fig.show()\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('loss.txt','r',encoding='utf-8') as f:\n",
    "    data=f.read()\n",
    "data=eval(data)\n",
    "fig=plt.figure()\n",
    "plt.plot([i*100 for i in range(len(data))],data)\n",
    "plt.xlabel('batch_num')\n",
    "plt.ylabel('loss')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ede3b1",
   "metadata": {},
   "source": [
    "## 20 加载之前的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2b1d09fc",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# en_path = r'H:\\datasets\\data\\翻译1\\test.en.txt'\n",
    "# ch_path = r'H:\\datasets\\data\\翻译1\\test.ch.txt'\n",
    "# tokenizer = Tokenizer(en_path, ch_path, count_min=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "51ea194b",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 数据评估\n",
    "def eval1():\n",
    "    device='cuda'\n",
    "    model1 = Transformer(tokenizer)\n",
    "    model1.load_state_dict(torch.load(f'./model/translation_25.pt'))\n",
    "    model1 = model1.to(device)\n",
    "    model1.eval()\n",
    "    all_=[]\n",
    "    for i in range(100):\n",
    "        random_integers = range(len(tokenizer.test))[i*10:i*10+10]  # 评估\n",
    "        end=compute_bleu4(tokenizer, random_integers, model1, device)\n",
    "        if end==0:\n",
    "            continue\n",
    "        all_.append(end)\n",
    "    print(sum(all_)/len(all_)) # 输出bleu4得分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9dff5b29",
   "metadata": {
    "metadata": {},
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './model/translation_25.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43meval1\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[27], line 5\u001b[0m, in \u001b[0;36meval1\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m model1 \u001b[38;5;241m=\u001b[39m Transformer(tokenizer)\n\u001b[0;32m----> 5\u001b[0m model1\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./model/translation_25.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      6\u001b[0m model1 \u001b[38;5;241m=\u001b[39m model1\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m model1\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/anaconda3/envs/prml/lib/python3.8/site-packages/torch/serialization.py:579\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    577\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    583\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    584\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/anaconda3/envs/prml/lib/python3.8/site-packages/torch/serialization.py:230\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 230\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/anaconda3/envs/prml/lib/python3.8/site-packages/torch/serialization.py:211\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_file, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './model/translation_25.pt'"
     ]
    }
   ],
   "source": [
    "eval1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73be28b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
