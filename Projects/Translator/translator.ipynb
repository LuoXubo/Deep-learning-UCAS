{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6016f24",
   "metadata": {},
   "source": [
    "## 1 导入相关包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af03343c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import math\n",
    "import torch\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import copy\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import sacrebleu\n",
    "import random\n",
    "import time\n",
    "import jieba\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # 下面老是报错 shape 不一致"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f5dbfd",
   "metadata": {},
   "source": [
    "## 2 定义dataset类\n",
    "用于传入dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d07e8aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    # 创建数据集\n",
    "    def __init__(self, src, tgt):\n",
    "        \"\"\"\n",
    "        初始化\n",
    "        :param src: 源数据(经tokenizer处理后)\n",
    "        :param tgt: 目标数据(经tokenizer处理后)\n",
    "        \"\"\"\n",
    "        self.src = src\n",
    "        self.tgt = tgt\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.src[i], self.tgt[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dab7e16",
   "metadata": {},
   "source": [
    "## 3 定义tokenizer类\n",
    "其作用是读取源数据并将其处理成可供模型输入的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6744d89b",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "class Tokenizer():\n",
    "    ## 定义tokenizer,对原始数据进行处理\n",
    "    def __init__(self, en_path, ch_path, count_min=5):\n",
    "        \"\"\"\n",
    "        初始化\n",
    "        :param en_path: 英文数据路径\n",
    "        :param ch_path: 中文数据路径\n",
    "        :param count_min: 对出现次数少于这个次数的数据进行过滤\n",
    "        \"\"\"\n",
    "        self.en_path = en_path  # 英文路径\n",
    "        self.ch_path = ch_path  # 中文路径\n",
    "        self.__count_min = count_min  # 对出现次数少于这个次数的数据进行过滤\n",
    "\n",
    "        # 读取原始英文数据\n",
    "        self.en_data = self.__read_ori_data(en_path)\n",
    "        # 读取原始中文数据\n",
    "        self.ch_data = self.__read_ori_data(ch_path)\n",
    "\n",
    "        self.index_2_word = ['unK', '<pad>', '<bos>', '<eos>']\n",
    "        self.word_2_index = {'unK': 0, '<pad>': 1, '<bos>': 2, '<eos>': 3}\n",
    "\n",
    "        self.en_set = set()\n",
    "        self.en_count = {}\n",
    "\n",
    "        # 中英文字符计数\n",
    "        self.__en_count = {}\n",
    "        self.__ch_count = {}\n",
    "\n",
    "        self.__count_word()\n",
    "        self.mx_length = 40\n",
    "        # 创建英文词汇表\n",
    "        self.data_ = []\n",
    "        self.__filter_data()\n",
    "        random.shuffle(self.data_)\n",
    "        self.test = self.data_[-1000:]\n",
    "        self.data_ = self.data_[:-1000]\n",
    "\n",
    "    def __read_ori_data(self, path):\n",
    "        \"\"\"\n",
    "        读取原始数据\n",
    "        :param path: 数据路径\n",
    "        :return: 返回一个列表，每个元素是一条数据\n",
    "        \"\"\"\n",
    "        print(path)\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            data = f.read().split('\\n')[:-1]\n",
    "        return data\n",
    "\n",
    "    def __count_word(self):\n",
    "        \"\"\"\n",
    "        统计中英文词汇表\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        le = len(self.en_data)\n",
    "        p = 0\n",
    "        # 统计英文词汇表\n",
    "        for data in self.en_data:\n",
    "            if p % 1000 == 0:\n",
    "                print('英文', p / le)\n",
    "            sentence = word_tokenize(data)\n",
    "            for sen in sentence:\n",
    "#                 sen=sen.lower()\n",
    "                if sen in self.en_set:\n",
    "                    self.en_count[sen] += 1\n",
    "                else:\n",
    "                    self.en_set.add(sen)\n",
    "                    self.en_count[sen] = 1\n",
    "            p += 1\n",
    "        for k, v in self.en_count.items():\n",
    "            if v >= self.__count_min:\n",
    "                self.word_2_index[k] = len(self.index_2_word)\n",
    "                self.index_2_word.append(k)\n",
    "            else:\n",
    "                self.word_2_index[k] = 0\n",
    "        self.en_set = set()\n",
    "        self.en_count = {}\n",
    "        p = 0\n",
    "        # 统计中文词汇表\n",
    "        for data in self.ch_data:\n",
    "            if p % 1000 == 0:\n",
    "                print('中文', p / le)\n",
    "            sentence = list(jieba.cut(data))\n",
    "            for sen in sentence:\n",
    "                if sen in self.en_set:\n",
    "                    self.en_count[sen] += 1\n",
    "                else:\n",
    "                    self.en_set.add(sen)\n",
    "                    self.en_count[sen] = 1\n",
    "            p += 1\n",
    "        # 构建词汇表\n",
    "        for k, v in self.en_count.items():\n",
    "            if v >= self.__count_min:\n",
    "                self.word_2_index[k] = len(self.index_2_word)\n",
    "                self.index_2_word.append(k)\n",
    "            else:\n",
    "                self.word_2_index[k] = 0\n",
    "\n",
    "    def __filter_data(self):\n",
    "        length = len(self.en_data)\n",
    "        for i in range(length):\n",
    "            # 0 代表英文到中文，1 代表中文到英文\n",
    "            self.data_.append([self.en_data[i], self.ch_data[i], 0])\n",
    "            self.data_.append([self.ch_data[i], self.en_data[i], 1])\n",
    "\n",
    "    def en_cut(self, data):\n",
    "        data = word_tokenize(data)\n",
    "        # 用于存放每个句子对应的编码\n",
    "        if len(data) > self.mx_length:\n",
    "            return 0, []\n",
    "        en_tokens = []\n",
    "        # 对分词结果进行遍历\n",
    "        for tk in data:\n",
    "#             x = tk.lower()\n",
    "            # 对于结果进行编码,0代表unK\n",
    "            en_tokens.append(self.word_2_index.get(tk, 0))\n",
    "        return 1, en_tokens\n",
    "\n",
    "    def ch_cut(self, data):\n",
    "        data = list(jieba.cut(data))\n",
    "#         list(data)[:-1]\n",
    "        # 用于存放每个句子对应的编码\n",
    "        if len(data) > self.mx_length:\n",
    "            return 0, []\n",
    "        en_tokens = []\n",
    "        # 对分词结果进行遍历\n",
    "        for tk in data:\n",
    "            # 对于结果进行编码,0代表unK\n",
    "            en_tokens.append(self.word_2_index.get(tk, 0))\n",
    "        return 1, en_tokens\n",
    "\n",
    "    def encode_all(self, data):\n",
    "        \"\"\"\n",
    "        对一组数据进行编码\n",
    "        :param data: data是一个数组，形状为n*3 每个元素是[src_sentence, tgt_sentence, label]，label 0 代表英文到中文，1 代表中文到英文\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        src = []\n",
    "        tgt = []\n",
    "        en_src, en_tgt, l = [], [], []\n",
    "        labels=[]\n",
    "        for i in data:\n",
    "            en_src.append(i[0])\n",
    "            en_tgt.append(i[1])\n",
    "            l.append(i[2])\n",
    "        for i in range(len(l)):\n",
    "            if l[i] == 0:\n",
    "                lab1, src_tokens = self.en_cut(en_src[i])\n",
    "                if lab1 == 0:\n",
    "                    continue\n",
    "                lab2, tgt_tokens = self.ch_cut(en_tgt[i])\n",
    "                if lab2 == 0:\n",
    "                    continue\n",
    "                src.append(src_tokens)\n",
    "                tgt.append(tgt_tokens)\n",
    "                labels.append(i)\n",
    "            else:\n",
    "                lab1, tgt_tokens = self.en_cut(en_tgt[i])\n",
    "                if lab1 == 0:\n",
    "                    continue\n",
    "                lab2, src_tokens = self.ch_cut(en_src[i])\n",
    "                if lab2 == 0:\n",
    "                    continue\n",
    "                src.append(src_tokens)\n",
    "                tgt.append(tgt_tokens)\n",
    "                labels.append(i)\n",
    "        return labels,src, tgt\n",
    "\n",
    "    def encode(self, src, l):\n",
    "        if l == 0:\n",
    "            src1 = word_tokenize(src)\n",
    "            # 用于存放每个句子对应的编码\n",
    "            en_tokens = []\n",
    "            # 对分词结果进行遍历\n",
    "            for tk in src1:\n",
    "#                 x = tk.lower()\n",
    "                # 对于结果进行编码\n",
    "                en_tokens.append(self.word_2_index.get(tk, 0))\n",
    "            return [en_tokens]\n",
    "        else:\n",
    "            src1 = list(jieba.cut(src))\n",
    "            # 用于存放每个句子对应的编码\n",
    "            en_tokens = []\n",
    "            # 对分词结果进行遍历\n",
    "            for tk in src1:\n",
    "                # 对于结果进行编码\n",
    "                en_tokens.append(self.word_2_index.get(tk, 0))\n",
    "            return [en_tokens]\n",
    "\n",
    "    def decode(self, data):\n",
    "        \"\"\"\n",
    "        数据解码\n",
    "        :param data: 这里传入一个中文的index\n",
    "        :return: 返回解码后的一个字符\n",
    "        \"\"\"\n",
    "        return self.index_2_word[data]\n",
    "\n",
    "    def __get_datasets(self, data):\n",
    "        \"\"\"\n",
    "        获取数据集\n",
    "        :return:返回DataSet类型的数据 或者 None\n",
    "        \"\"\"\n",
    "        # 将数据编码并\n",
    "        labels,src, tgt = self.encode_all(data)\n",
    "        # 返回数据集\n",
    "        return TranslationDataset(src, tgt)\n",
    "\n",
    "    def another_process(self, batch_datas):\n",
    "        \"\"\"\n",
    "        特殊处理，这里传入一个batch的数据，并对这个batch的数据进行填充，使得每一行的数据长度相同。这里填充pad 空字符  bos 开始  eos结束\n",
    "        :param batch_datas: 一个batch的数据\n",
    "        :return: 返回填充后的数据\n",
    "        \"\"\"\n",
    "        # 创建四个空字典存储数据\n",
    "        en_index, ch_index = [], []  # 中文英文索引，中文索引\n",
    "        en_len, ch_len = [], []  # 没行英文长度，每行中文长度\n",
    "\n",
    "        for en, ch in batch_datas:  # 对batch进行遍历，将所有数据的索引与长度加入四个列表\n",
    "            en_index.append(en)\n",
    "            ch_index.append(ch)\n",
    "            en_len.append(len(en))\n",
    "            ch_len.append(len(ch))\n",
    "\n",
    "        # 获取中英文的最大长度，根据这个长度对所有数据进行填充，使每行数据长度相同\n",
    "        max_en_len = max(en_len)\n",
    "        max_ch_len = max(ch_len)\n",
    "        max_len = max(max_en_len, max_ch_len + 2)\n",
    "\n",
    "        # 英文数据填充，i是原始数据，后面是填充的pad\n",
    "        en_index = [i + [self.word_2_index['<pad>']] * (max_len - len(i)) for i in en_index]\n",
    "        # 中文数据填充 先填充bos表示句子开始，后面接原始数据，最后填充eos表示句子结束，后面接pad\n",
    "        ch_index = [[self.word_2_index['<bos>']] + i + [self.word_2_index['<eos>']] +\n",
    "                    [self.word_2_index['<pad>']] * (max_len - len(i) + 1) for i in ch_index]\n",
    "\n",
    "        # 将处理后的数据转换为tensor并放到相应设备上\n",
    "        en_index = torch.tensor(en_index)\n",
    "        ch_index = torch.tensor(ch_index)\n",
    "        return en_index, ch_index\n",
    "\n",
    "    def get_dataloader(self, data, batch_size=40):\n",
    "        \"\"\"\n",
    "        获取dataloader\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # 获取数据集\n",
    "        data = self.__get_datasets(data)\n",
    "        # 返回DataLoader类型的数据\n",
    "        return DataLoader(data, batch_size=batch_size, shuffle=True, collate_fn=self.another_process)\n",
    "\n",
    "    # 获取英文词表大小\n",
    "    def get_vocab_size(self):\n",
    "        return len(self.index_2_word)\n",
    "\n",
    "    # 获取数据集大小\n",
    "    def get_dataset_size(self):\n",
    "        return len(self.en_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f6d085",
   "metadata": {},
   "source": [
    "## 4 定义batch类\n",
    "其作用是生成掩码与统计非填充字符数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f84ee93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    # 批次类,对每一个批次的数据进行掩码生成操作\n",
    "    def __init__(self, src, trg=None, tokenizer=None, device='cuda'):\n",
    "        \"\"\"\n",
    "        初始化函数\n",
    "        :param src: 源数据\n",
    "        :param trg: 目标数据\n",
    "        :param tokenizer: 分词器\n",
    "        :param device: 训练设备\n",
    "        \"\"\"\n",
    "        # 将输入、输出单词id表示的数据规范成整数类型并转换到训练设备上\n",
    "        src = src.to(device).long()\n",
    "        trg = trg.to(device).long()\n",
    "        self.src = src  # 源数据 (batch, seq_len)\n",
    "        self.__pad = tokenizer.word_2_index['<pad>']  # 填充字符的索引\n",
    "        # 对于当前输入的语句非空部分进行判断，这里是对源数据进行掩码操作，将填充的内容置为0\n",
    "        # 并在seq length前面增加一维，形成维度为 1×seq length 的矩阵\n",
    "        self.src_mask = (src != self.__pad).unsqueeze(-2)\n",
    "        # 如果输出目标不为空，则需要对解码器使用的目标语句进行掩码\n",
    "        if trg is not None:\n",
    "            # 解码器使用的目标输入部分\n",
    "            self.trg = trg[:, : -1]\n",
    "            # 解码器训练时应预测输出的目标结果\n",
    "            self.trg_y = trg[:, 1:]\n",
    "            # 将目标输入部分进行注意力掩码\n",
    "            self.trg_mask = self.make_std_mask(self.trg, self.__pad)\n",
    "            # 将应输出的目标结果中实际的词数进行统计\n",
    "            self.ntokens = (self.trg_y != self.__pad).data.sum()\n",
    "\n",
    "    # 掩码操作\n",
    "    @staticmethod\n",
    "    def make_std_mask(tgt, pad):\n",
    "        \"\"\"\n",
    "        生成掩码矩阵\n",
    "        :param tgt: 目标数据\n",
    "        :param pad: 填充字符的索引\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)  # 首先对pad进行掩码生成\n",
    "        # 这里对注意力进行掩码操作并与pad掩码结合起来。\n",
    "        tgt_mask = tgt_mask & Variable(subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
    "        return tgt_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4b2266",
   "metadata": {},
   "source": [
    "#### 注意力掩码生成函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26bfeea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"\"\"\n",
    "    注意力机制掩码生成\n",
    "    :param size: 句子长度\n",
    "    :return: 注意力掩码\n",
    "    \"\"\"\n",
    "    # 设定subsequent_mask矩阵的shape\n",
    "    attn_shape = (1, size, size)\n",
    "    # 生成一个右上角(不含主对角线)为全1，左下角(含主对角线)为全0的subsequent_mask矩阵\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    # 返回一个右上角(不含主对角线)为全False，左下角(含主对角线)为全True的subsequent_mask矩阵\n",
    "    return torch.from_numpy(subsequent_mask) == 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97df4f92",
   "metadata": {},
   "source": [
    "## 5 词嵌入类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcaf8597",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    # 词嵌入层\n",
    "    def __init__(self, d_model, vocab):\n",
    "        \"\"\"\n",
    "        词嵌入层初始化\n",
    "        :param d_model: 词嵌入维度\n",
    "        :param vocab: 词表大小\n",
    "        \"\"\"\n",
    "        super(Embedding, self).__init__()\n",
    "        # Embedding层\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        # Embedding维数\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 返回x的词向量（需要乘以math.sqrt(d_model)）\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fccb95",
   "metadata": {},
   "source": [
    "## 6 位置编码器类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0c071d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    # 位置编码器层\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000, device='cuda'):\n",
    "        \"\"\"\n",
    "        位置编码器层初始化\n",
    "        :param d_model: 词嵌入维度\n",
    "        :param dropout: dropout比例\n",
    "        :param max_len: 序列最大长度\n",
    "        :param device: 训练设备\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        # 位置编码矩阵，维度[max_len, embedding_dim]\n",
    "        pe = torch.zeros(max_len, d_model, device=device)\n",
    "        # 单词位置\n",
    "        position = torch.arange(0.0, max_len, device=device)\n",
    "        position.unsqueeze_(1)\n",
    "        # 使用exp和log实现幂运算\n",
    "        div_term = torch.exp(torch.arange(0.0, d_model, 2, device=device) * (- math.log(1e4) / d_model))\n",
    "        div_term.unsqueeze_(0)\n",
    "        # 计算单词位置沿词向量维度的纹理值\n",
    "        pe[:, 0:: 2] = torch.sin(torch.mm(position, div_term))\n",
    "        pe[:, 1:: 2] = torch.cos(torch.mm(position, div_term))\n",
    "        # 增加批次维度，[1, max_len, embedding_dim]\n",
    "        pe.unsqueeze_(0)\n",
    "        # 将位置编码矩阵注册为buffer(不参加训练)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 将一个批次中语句所有词向量与位置编码相加\n",
    "        # 注意，位置编码不参与训练，因此设置requires_grad=False\n",
    "        x += Variable(self.pe[:, : x.size(1), :], requires_grad=False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82d6f3a",
   "metadata": {},
   "source": [
    "## 7 多头注意力机制类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d0349f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    # 多头注意力机制\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"\"\"\n",
    "        多头注意力机制初始化\n",
    "        :param h: 多头\n",
    "        :param d_model: 词嵌入维度\n",
    "        :param dropout: dropout比例\n",
    "        \"\"\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        # 确保整除\n",
    "        assert d_model % h == 0\n",
    "        # q、k、v向量维数\n",
    "        self.d_k = d_model // h\n",
    "        # 头的数量\n",
    "        self.h = h\n",
    "        # WQ、WK、WV矩阵及多头注意力拼接变换矩阵WO 4个线性层\n",
    "        self.linears = nn.ModuleList([copy.deepcopy(nn.Linear(d_model, d_model)) for _ in range(4)])\n",
    "        # 注意力机制函数\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        反向传播\n",
    "        :param query: q\n",
    "        :param key: k\n",
    "        :param value: v\n",
    "        :param mask: 掩码\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "        # 批次大小\n",
    "        nbatches = query.size(0)\n",
    "        # WQ、WK、WV分别对词向量线性变换，并将结果拆成h块\n",
    "        query, key, value = [\n",
    "            l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "            for l, x in zip(self.linears, (query, key, value))\n",
    "        ]\n",
    "        # 注意力加权\n",
    "        x, self.attn = self.attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "        # 多头注意力加权拼接\n",
    "        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)\n",
    "        # 对多头注意力加权拼接结果线性变换\n",
    "        return self.linears[-1](x)\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask=None, dropout=None):\n",
    "        \"\"\"\n",
    "        注意力加权\n",
    "        :param query: q\n",
    "        :param key: k\n",
    "        :param value: v\n",
    "        :param mask: 掩码矩阵\n",
    "        :param dropout: dropout比例\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # q、k、v向量长度为d_k\n",
    "        d_k = query.size(-1)\n",
    "        # 矩阵乘法实现q、k点积注意力，sqrt(d_k)归一化\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        # 注意力掩码机制\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        # 注意力矩阵softmax归一化\n",
    "        p_attn = F.softmax(scores, dim=-1)\n",
    "        # dropout\n",
    "        if dropout is not None:\n",
    "            p_attn = dropout(p_attn)\n",
    "        # 注意力对v加权\n",
    "        return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929c47ed",
   "metadata": {},
   "source": [
    "## 8 子层连接结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6de60f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    # 子层连接结构 用于连接注意力机制以及前馈全连接网络\n",
    "    def __init__(self, d_model, dropout):\n",
    "        \"\"\"\n",
    "        子层连接结构初始化层\n",
    "        :param d_model: 词嵌入纬度\n",
    "        :param dropout: dropout比例\n",
    "        \"\"\"\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        # 规范化层\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        # dropout层\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        # 层归一化\n",
    "        x_ = self.norm(x)\n",
    "        x_ = sublayer(x_)\n",
    "        x_ = self.dropout(x_)\n",
    "        # 残差连接\n",
    "        return x + x_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf100c0",
   "metadata": {},
   "source": [
    "## 9 前馈全连接网络类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5a692cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    # 前馈全连接网络\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        前馈全连接网络初始化层\n",
    "        :param d_model: 词嵌入维度\n",
    "        :param d_ff: 中间隐层维度\n",
    "        :param dropout: dropout比例\n",
    "        \"\"\"\n",
    "        super(FeedForward, self).__init__()\n",
    "        # 全连接层\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        # Dropout层\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.w_1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.w_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b592b9ca",
   "metadata": {},
   "source": [
    "## 10 编码器类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acb56fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    # 编码器\n",
    "    def __init__(self, h, d_model, d_ff=256, dropout=0.1):\n",
    "        \"\"\"\n",
    "        编码器层初始化\n",
    "        :param h: 头数\n",
    "        :param d_model: 词嵌入维度\n",
    "        :param d_ff: 中间隐层维度\n",
    "        :param dropout: dropout比例\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        # 多头注意力\n",
    "        self.self_attn = MultiHeadedAttention(h, d_model)\n",
    "        # 前馈全连接层\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        # 子层连接结构\n",
    "        self.sublayer1 = SublayerConnection(d_model, dropout)\n",
    "        self.sublayer2 = SublayerConnection(d_model, dropout)\n",
    "        # 规范化层\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # 将embedding层进行Multi head Attention\n",
    "        x = self.sublayer1(x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        # attn的结果直接作为下一层输入\n",
    "        return self.norm(self.sublayer2(x, self.feed_forward))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82943b33",
   "metadata": {},
   "source": [
    "## 11 解码器类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "facdc3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, h, d_model, d_ff=256, dropout=0.1):\n",
    "        \"\"\"\n",
    "        解码器层\n",
    "        :param h: 头数\n",
    "        :param d_model: 词嵌入维度\n",
    "        :param d_ff: 中间隐层维度\n",
    "        :param dropout: dropout比例\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        self.size = d_model\n",
    "        # 自注意力机制\n",
    "        self.self_attn = MultiHeadedAttention(h, d_model)\n",
    "        # 上下文注意力机制\n",
    "        self.src_attn = MultiHeadedAttention(h, d_model)\n",
    "        # 前馈全连接子层\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        # 子层连接结构\n",
    "        self.sublayer1 = SublayerConnection(d_model, dropout)\n",
    "        self.sublayer2 = SublayerConnection(d_model, dropout)\n",
    "        self.sublayer3 = SublayerConnection(d_model, dropout)\n",
    "        # 规范化层\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        # memory为编码器输出隐表示\n",
    "        m = memory\n",
    "        # 自注意力机制，q、k、v均来自解码器隐表示\n",
    "        x = self.sublayer1(x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        # 上下文注意力机制：q为来自解码器隐表示，而k、v为编码器隐表示\n",
    "        x = self.sublayer2(x, lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        return self.norm(self.sublayer3(x, self.feed_forward))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8215038",
   "metadata": {},
   "source": [
    "## 12 生成器类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c09d0aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    #  生成器层\n",
    "    def __init__(self, d_model, vocab):\n",
    "        \"\"\"\n",
    "        生成器层初始化\n",
    "        :param d_model:\n",
    "        :param vocab:\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "        # decode后的结果，先进入一个全连接层变为词典大小的向量\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 然后再进行log_softmax操作(在softmax结果上再做多一次log运算)\n",
    "        return F.log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d318d0eb",
   "metadata": {},
   "source": [
    "## 13 transformer框架"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00026630",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    # Transformer层\n",
    "    def __init__(self, tokenizer, h=8, d_model=256, E_N=2, D_N=2, device='cuda'):\n",
    "        \"\"\"\n",
    "        transformer层初始化\n",
    "        :param h: 头数\n",
    "        :param d_model: 词嵌入纬度\n",
    "        :param tokenizer:\n",
    "        :param E_N:\n",
    "        :param D_N:\n",
    "        :param device:\n",
    "        \"\"\"\n",
    "        super(Transformer, self).__init__()\n",
    "        # 编码器\n",
    "        self.encoder = nn.ModuleList([Encoder(h, d_model) for _ in range(E_N)])\n",
    "        # 解码器\n",
    "        self.decoder = nn.ModuleList([Decoder(h, d_model) for _ in range(D_N)])\n",
    "        # 词嵌入层\n",
    "        self.src_embed = Embedding(d_model, tokenizer.get_vocab_size())\n",
    "        self.tgt_embed = Embedding(d_model, tokenizer.get_vocab_size())\n",
    "        # 位置编码器层\n",
    "        self.src_pos = PositionalEncoding(d_model, device=device)\n",
    "        self.tgt_pos = PositionalEncoding(d_model, device=device)\n",
    "        # 生成器层\n",
    "        self.generator = Generator(d_model, tokenizer.get_vocab_size())\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        \"\"\"\n",
    "        编码\n",
    "        :param src: 源数据\n",
    "        :param src_mask: 源数据掩码\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        \n",
    "        # 词嵌入\n",
    "        src = self.src_embed(src)\n",
    "        # 位置编码\n",
    "        src = self.src_pos(src)\n",
    "        # 编码\n",
    "        for i in self.encoder:\n",
    "            src = i(src, src_mask)\n",
    "        return src\n",
    "\n",
    "    def decode(self, memory, tgt, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "        解码\n",
    "        :param memory: 编码器输出\n",
    "        :param tgt: 目标数据输入\n",
    "        :param src_mask: 源数据掩码\n",
    "        :param tgt_mask: 目标数据掩码\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        #  词嵌入\n",
    "        tgt = self.tgt_embed(tgt)\n",
    "        #  位置编码\n",
    "        tgt = self.tgt_pos(tgt)\n",
    "        # 解码\n",
    "        for i in self.decoder:\n",
    "            tgt = i(tgt, memory, src_mask, tgt_mask)\n",
    "        return tgt\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "        反向传播\n",
    "        :param src: 源数据\n",
    "        :param tgt: 目标数据\n",
    "        :param src_mask: 源数据掩码\n",
    "        :param tgt_mask: 目标数据掩码\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # encoder的结果作为decoder的memory参数传入，进行decode\n",
    "        return self.decode(self.encode(src, src_mask), tgt, src_mask, tgt_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf8b6f2",
   "metadata": {},
   "source": [
    "## 14 标签平滑类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b02006b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    # 标签平滑\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        \"\"\"\n",
    "        初始化\n",
    "        :param size: 目标数据词表大小\n",
    "        :param padding_idx: 目标数据填充字符的索引\n",
    "        :param smoothing: 做平滑的值，为0即不进行平滑\n",
    "        \"\"\"\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        # KL散度，通常用于测量两个概率分布之间的差异\n",
    "        self.criterion = nn.KLDivLoss(reduction='sum')\n",
    "        # 目标数据填充字符的索引\n",
    "        self.padding_idx = padding_idx\n",
    "        # 置信度\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        # 平滑值\n",
    "        self.smoothing = smoothing\n",
    "        # 词表大小\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        \"\"\"\n",
    "        反向传播\n",
    "        :param x: 预测值\n",
    "        :param target: 目标值\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # 判断输出值的第二维传长度是否等于输出词表的大小，这里x的shape为 （batch*seqlength,x.shape(-1)）\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        # 标签平滑填充\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        # 这里的操作是将真实值的位置进行替换,替换成置信度\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        # 将填充的位置的值设置为0\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        # 生成填充部分的掩码\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        self.true_dist = true_dist\n",
    "        # 返回KL散度\n",
    "        return self.criterion(x, Variable(true_dist, requires_grad=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5ab2a5",
   "metadata": {},
   "source": [
    "## 15 损失计算类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c0f66c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLossCompute:\n",
    "    # 计算损失和进行参数反向传播\n",
    "    def __init__(self, generator, criterion, opt=None):\n",
    "        \"\"\"\n",
    "        初始化\n",
    "        :param generator: 生成器，transformer模块中的最后一层，这里将其单独拿出来而不直接放进transformer中的原因是：\n",
    "            预测数据的是时候，我们需要利用之前的结果，但是我们只去最后一个作为本次输出，那么在进行输出时，只对最后一个进行输出，单独拿出来进行输出的线性变换，更灵活\n",
    "        :param criterion: 标签平滑的类\n",
    "        :param opt: 经wormup后的optimizer\n",
    "        \"\"\"\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "        self.opt = opt\n",
    "\n",
    "    def __call__(self, x, y, norm):\n",
    "        \"\"\"\n",
    "        类做函数调用\n",
    "        :param x: 经transformer解码后的结果\n",
    "        :param y: 目标值\n",
    "        :param norm: 本次数据有效的字符数，即，除去padding后的字符数\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # 进行输出\n",
    "        x = self.generator(x)\n",
    "        # 得到KL散度\n",
    "        loss = self.criterion(x.contiguous().view(-1, x.size(-1)),\n",
    "                              y.contiguous().view(-1)) / norm\n",
    "        # 反向椽笔\n",
    "        loss.backward()\n",
    "        if self.opt is not None:\n",
    "            # 参数更新\n",
    "            self.opt.step()\n",
    "            # 优化器梯度置0\n",
    "            self.opt.optimizer.zero_grad()\n",
    "        # 返回损失\n",
    "        return loss.data.item() * norm.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea106b5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d01a5bc1",
   "metadata": {},
   "source": [
    "## 16 Warmup-学习率更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c275e7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoamOpt:\n",
    "    # warmup\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        \"\"\"\n",
    "        初始化\n",
    "        :param model_size: 词嵌入维度\n",
    "        :param factor:\n",
    "        :param warmup:\n",
    "        :param optimizer:\n",
    "        \"\"\"\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "\n",
    "    def step(self):\n",
    "        # 学习率更新\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "    def rate(self, step=None):\n",
    "        # 学习率更新函数\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * (self.model_size ** (-0.5) * min(step ** (-0.5), step * self.warmup ** (-1.5)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04bf5a5",
   "metadata": {},
   "source": [
    "## 评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a9fb0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_english_sentence(sentence):\n",
    "    # 使用正则表达式检查句子中是否包含英文字母\n",
    "    english_pattern = re.compile(r'[a-zA-Z]')\n",
    "    match = english_pattern.search(sentence)\n",
    "    # True 表示这是英文句子\n",
    "    if match: \n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "# 这个smooth防止句子长度小于4而出现报错\n",
    "smooth = SmoothingFunction().method1\n",
    "def compute_bleu4(tokenizer, random_integers, model, device):\n",
    "    \"\"\"\n",
    "    计算BLEU4\n",
    "    :param tokenizer: tokenizer\n",
    "    :param random_integers: 这个是随机选择的测试集数据的编号\n",
    "    :param model: 模型\n",
    "    :param device: 设备\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # m1,m2存放英文的原数据与模型输出数据\n",
    "    m1, m2 = [], []\n",
    "    # m3,m4存放英文的原数据与模型输出数据\n",
    "    m3, m4 = [], []\n",
    "    model.eval()\n",
    "    # 存放测试数据\n",
    "    da = []\n",
    "    # 将随机选择的测试集数据编号添加到da中\n",
    "    for i in random_integers:\n",
    "        da.append(tokenizer.test[i])\n",
    "    # 对da中的数据进行编码\n",
    "    labels, x, _ = tokenizer.encode_all(da)\n",
    "    with torch.no_grad():\n",
    "        # 预测\n",
    "        y = predict(x, model, tokenizer, device)\n",
    "    # 这个p用于记录y的索引\n",
    "    p = 0\n",
    "    # 用于保存有效的索引\n",
    "    itg = []\n",
    "    # 这里我限制输入数据全部有效，如果有无效的数据，直接放弃本次计算\n",
    "    if len(y) != 10:\n",
    "        return 0\n",
    "    for i in labels:\n",
    "        # 取出有效的索引\n",
    "        itg.append(random_integers[i])\n",
    "    # 将真实数据与预测数据分别放到m1,m2,m3,m4中\n",
    "    for i in itg:\n",
    "        if is_english_sentence(tokenizer.test[i][1]):\n",
    "            m1.append(tokenizer.test[i][1])\n",
    "            m2.append([y[p]])\n",
    "        else:\n",
    "            m3.append(list(jieba.cut(tokenizer.test[i][1])))\n",
    "            m4.append([list(jieba.cut(y[p]))])\n",
    "        p += 1\n",
    "    smooth = SmoothingFunction().method1\n",
    "    # 计算英文的bleu4\n",
    "    b1 = [sacrebleu.sentence_bleu(candidate, refs).score for candidate, refs in zip(m1, m2)]\n",
    "    # 计算中文的bleu4\n",
    "    for i in range(len(m4)):\n",
    "        b2 = sentence_bleu(m4[i], m3[i], weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smooth) * 100\n",
    "        b1.append(b2)\n",
    "#     print(b1)\n",
    "#     print(sum(b1)/len(b1))\n",
    "    return sum(b1)/len(b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb47ba2",
   "metadata": {},
   "source": [
    "## 17 训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c384beae",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/xubo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/xubo/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./src.txt\n",
      "./tgt.txt\n",
      "英文 0.0\n",
      "英文 0.014628651677174915\n",
      "英文 0.02925730335434983\n",
      "英文 0.043885955031524745\n",
      "英文 0.05851460670869966\n",
      "英文 0.07314325838587457\n",
      "英文 0.08777191006304949\n",
      "英文 0.1024005617402244\n",
      "英文 0.11702921341739932\n",
      "英文 0.13165786509457422\n",
      "英文 0.14628651677174914\n",
      "英文 0.16091516844892406\n",
      "英文 0.17554382012609898\n",
      "英文 0.1901724718032739\n",
      "英文 0.2048011234804488\n",
      "英文 0.2194297751576237\n",
      "英文 0.23405842683479863\n",
      "英文 0.24868707851197355\n",
      "英文 0.26331573018914844\n",
      "英文 0.27794438186632336\n",
      "英文 0.2925730335434983\n",
      "英文 0.3072016852206732\n",
      "英文 0.3218303368978481\n",
      "英文 0.33645898857502304\n",
      "英文 0.35108764025219796\n",
      "英文 0.3657162919293729\n",
      "英文 0.3803449436065478\n",
      "英文 0.3949735952837227\n",
      "英文 0.4096022469608976\n",
      "英文 0.4242308986380725\n",
      "英文 0.4388595503152474\n",
      "英文 0.45348820199242235\n",
      "英文 0.46811685366959727\n",
      "英文 0.4827455053467722\n",
      "英文 0.4973741570239471\n",
      "英文 0.512002808701122\n",
      "英文 0.5266314603782969\n",
      "英文 0.5412601120554719\n",
      "英文 0.5558887637326467\n",
      "英文 0.5705174154098217\n",
      "英文 0.5851460670869966\n",
      "英文 0.5997747187641715\n",
      "英文 0.6144033704413464\n",
      "英文 0.6290320221185214\n",
      "英文 0.6436606737956962\n",
      "英文 0.6582893254728711\n",
      "英文 0.6729179771500461\n",
      "英文 0.687546628827221\n",
      "英文 0.7021752805043959\n",
      "英文 0.7168039321815708\n",
      "英文 0.7314325838587458\n",
      "英文 0.7460612355359206\n",
      "英文 0.7606898872130956\n",
      "英文 0.7753185388902705\n",
      "英文 0.7899471905674454\n",
      "英文 0.8045758422446203\n",
      "英文 0.8192044939217952\n",
      "英文 0.8338331455989701\n",
      "英文 0.848461797276145\n",
      "英文 0.86309044895332\n",
      "英文 0.8777191006304949\n",
      "英文 0.8923477523076698\n",
      "英文 0.9069764039848447\n",
      "英文 0.9216050556620197\n",
      "英文 0.9362337073391945\n",
      "英文 0.9508623590163695\n",
      "英文 0.9654910106935444\n",
      "英文 0.9801196623707192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "英文 0.9947483140478942\n",
      "中文 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.931 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中文 0.014628651677174915\n",
      "中文 0.02925730335434983\n",
      "中文 0.043885955031524745\n",
      "中文 0.05851460670869966\n",
      "中文 0.07314325838587457\n",
      "中文 0.08777191006304949\n",
      "中文 0.1024005617402244\n",
      "中文 0.11702921341739932\n",
      "中文 0.13165786509457422\n",
      "中文 0.14628651677174914\n",
      "中文 0.16091516844892406\n",
      "中文 0.17554382012609898\n",
      "中文 0.1901724718032739\n",
      "中文 0.2048011234804488\n",
      "中文 0.2194297751576237\n",
      "中文 0.23405842683479863\n",
      "中文 0.24868707851197355\n",
      "中文 0.26331573018914844\n",
      "中文 0.27794438186632336\n",
      "中文 0.2925730335434983\n",
      "中文 0.3072016852206732\n",
      "中文 0.3218303368978481\n",
      "中文 0.33645898857502304\n",
      "中文 0.35108764025219796\n",
      "中文 0.3657162919293729\n",
      "中文 0.3803449436065478\n",
      "中文 0.3949735952837227\n",
      "中文 0.4096022469608976\n",
      "中文 0.4242308986380725\n",
      "中文 0.4388595503152474\n",
      "中文 0.45348820199242235\n",
      "中文 0.46811685366959727\n",
      "中文 0.4827455053467722\n",
      "中文 0.4973741570239471\n",
      "中文 0.512002808701122\n",
      "中文 0.5266314603782969\n",
      "中文 0.5412601120554719\n",
      "中文 0.5558887637326467\n",
      "中文 0.5705174154098217\n",
      "中文 0.5851460670869966\n",
      "中文 0.5997747187641715\n",
      "中文 0.6144033704413464\n",
      "中文 0.6290320221185214\n",
      "中文 0.6436606737956962\n",
      "中文 0.6582893254728711\n",
      "中文 0.6729179771500461\n",
      "中文 0.687546628827221\n",
      "中文 0.7021752805043959\n",
      "中文 0.7168039321815708\n",
      "中文 0.7314325838587458\n",
      "中文 0.7460612355359206\n",
      "中文 0.7606898872130956\n",
      "中文 0.7753185388902705\n",
      "中文 0.7899471905674454\n",
      "中文 0.8045758422446203\n",
      "中文 0.8192044939217952\n",
      "中文 0.8338331455989701\n",
      "中文 0.848461797276145\n",
      "中文 0.86309044895332\n",
      "中文 0.8777191006304949\n",
      "中文 0.8923477523076698\n",
      "中文 0.9069764039848447\n",
      "中文 0.9216050556620197\n",
      "中文 0.9362337073391945\n",
      "中文 0.9508623590163695\n",
      "中文 0.9654910106935444\n",
      "中文 0.9801196623707192\n",
      "中文 0.9947483140478942\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('words')\n",
    "en_path = r'./src.txt'\n",
    "ch_path = r'./tgt.txt'\n",
    "tokenizer = Tokenizer(en_path, ch_path, count_min=3)\n",
    "# 训练\n",
    "def train(): \n",
    "    device = 'cuda'\n",
    "    model = Transformer(tokenizer, device=device)\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            # 这里初始化采用的是nn.init.xavier_uniform\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    model = model.to(device)\n",
    "    criteria = LabelSmoothing(tokenizer.get_vocab_size(), tokenizer.word_2_index['<pad>'])\n",
    "    optimizer = NoamOpt(256, 1, 2000,\n",
    "                        torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
    "    lossF = SimpleLossCompute(model.generator, criteria, optimizer)\n",
    "    epochs = 100\n",
    "    model.train()\n",
    "    loss_all = []\n",
    "    print('词表大小', tokenizer.get_vocab_size())\n",
    "    t = time.time()\n",
    "    data_loader = tokenizer.get_dataloader(tokenizer.data_)\n",
    "    random_integers = random.sample(range(len(tokenizer.test)-10), 6)  # 随机选100个句子\n",
    "    batchs=[]\n",
    "    for index, data in enumerate(data_loader):\n",
    "        src, tgt = data\n",
    "        # 处理一个batch\n",
    "        batch = Batch(src, tgt, tokenizer=tokenizer, device=device)\n",
    "        batchs.append(batch)\n",
    "    for epoch in range(epochs):\n",
    "        p=0\n",
    "        for batch in batchs:\n",
    "            out = model(batch.src, batch.trg, batch.src_mask, batch.trg_mask)\n",
    "            out = lossF(out, batch.trg_y, batch.ntokens)\n",
    "            if (p+1) % 1000 == 0:\n",
    "                model.eval()\n",
    "#                 compute_bleu4(tokenizer, random_integers, model, device)\n",
    "                print('epoch', epoch, 'loss', float(out / batch.ntokens))\n",
    "                model.train()\n",
    "                print('time', time.time() - t)\n",
    "                if float(out / batch.ntokens)<2.2:\n",
    "                    random_integers = random.sample(range(len(tokenizer.test)), 100)\n",
    "                    nu=compute_bleu4(tokenizer, random_integers, model, device)\n",
    "                    if nu > 17:\n",
    "                        torch.save(model.state_dict(), f'./model/translation_{epoch}_{p}.pt')\n",
    "                        break\n",
    "                    if nu > 14:\n",
    "                        torch.save(model.state_dict(), f'./model/translation_{epoch}_{p}.pt')\n",
    "\n",
    "            if p%100==0:\n",
    "                print(p/1000)\n",
    "            p+=1\n",
    "        \n",
    "        loss_all.append(float(out / batch.ntokens))\n",
    "        \n",
    "        \n",
    "\n",
    "    with open('loss.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write(str(loss_all))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59aecc9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "298abc2a",
   "metadata": {},
   "source": [
    "## 18 预测函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af7ad45d",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import words\n",
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    \"\"\"\n",
    "    传入一个训练好的模型，对指定数据进行预测\n",
    "    \"\"\"\n",
    "    # 先用encoder进行encode\n",
    "    memory = model.encode(src, src_mask)\n",
    "    # 初始化预测内容为1×1的tensor，填入开始符('BOS')的id，并将type设置为输入数据类型(LongTensor)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
    "    # 遍历输出的长度下标\n",
    "    for i in range(max_len - 1):\n",
    "        # decode得到隐层表示\n",
    "        out = model.decode(memory,\n",
    "                           Variable(ys),\n",
    "                           src_mask,\n",
    "                           Variable(subsequent_mask(ys.size(1)).type_as(src.data)))\n",
    "        # 将隐藏表示转为对词典各词的log_softmax概率分布表示\n",
    "        prob = model.generator(out[:, i])\n",
    "        # 获取当前位置最大概率的预测词id\n",
    "        _, next_word = torch.max(prob, dim=-1)\n",
    "        next_word = next_word.data[0]\n",
    "        # 将当前位置预测的字符id与之前的预测内容拼接起来\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
    "    return ys\n",
    "\n",
    "\n",
    "def predict(data, model, tokenizer, device='cuda'):\n",
    "    \"\"\"\n",
    "    在data上用训练好的模型进行预测，打印模型翻译结果\n",
    "    \"\"\"\n",
    "    # 梯度清零\n",
    "    with torch.no_grad():\n",
    "        # 在data的英文数据长度上遍历下标\n",
    "        data1=[]\n",
    "        for i in range(len(data)):\n",
    "            # 打印待翻译的英文语句\n",
    "\n",
    "            # 将当前以单词id表示的英文语句数据转为tensor，并放如DEVICE中\n",
    "            src = torch.from_numpy(np.array(data[i])).long().to(device)\n",
    "            # 增加一维\n",
    "            src = src.unsqueeze(0)\n",
    "            # 设置attention mask\n",
    "            src_mask = (src != tokenizer.word_2_index['<pad>']).unsqueeze(-2)\n",
    "            # 用训练好的模型进行decode预测\n",
    "            out = greedy_decode(model, src, src_mask, max_len=100, start_symbol=tokenizer.word_2_index['<bos>'])\n",
    "            # 初始化一个用于存放模型翻译结果语句单词的列表\n",
    "            translation = []\n",
    "            # 遍历翻译输出字符的下标（注意：开始符\"BOS\"的索引0不遍历）\n",
    "            for j in range(1, out.size(1)):\n",
    "                # 获取当前下标的输出字符\n",
    "                sym = tokenizer.index_2_word[out[0, j].item()]\n",
    "                # 如果输出字符不为'EOS'终止符，则添加到当前语句的翻译结果列表\n",
    "                if sym != '<eos>':\n",
    "                    translation.append(sym)\n",
    "                # 否则终止遍历\n",
    "                else:\n",
    "                    break\n",
    "            # 打印模型翻译输出的中文语句结果\n",
    "            if len(translation)>0:\n",
    "                if translation[0].lower() in words.words():\n",
    "                    data1.append(TreebankWordDetokenizer().detokenize(translation))\n",
    "                else:\n",
    "                    data1.append(\"\".join(translation))\n",
    "        return data1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54989ea",
   "metadata": {},
   "source": [
    "## 19 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2496758e",
   "metadata": {
    "metadata": {},
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词表大小 21270\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 0 loss 4.689365863800049\n",
      "time 324.37181186676025\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 0 loss 4.431013584136963\n",
      "time 613.7734763622284\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 0 loss 4.472936153411865\n",
      "time 900.9064693450928\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 1 loss 4.207980155944824\n",
      "time 1300.007234096527\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 1 loss 3.9414854049682617\n",
      "time 1577.575853586197\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 1 loss 4.065410137176514\n",
      "time 1854.8221833705902\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 2 loss 3.925121784210205\n",
      "time 2240.2356774806976\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 2 loss 3.767831563949585\n",
      "time 2518.314291715622\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 2 loss 3.9571712017059326\n",
      "time 2794.4452707767487\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 3 loss 3.6374330520629883\n",
      "time 3179.4317157268524\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 3 loss 3.7384917736053467\n",
      "time 3457.329729795456\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 3 loss 3.7989346981048584\n",
      "time 3734.7166769504547\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 4 loss 3.491396188735962\n",
      "time 4121.807061433792\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 4 loss 3.6034462451934814\n",
      "time 4398.704243421555\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 4 loss 3.7373783588409424\n",
      "time 4676.168776273727\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 5 loss 3.444524049758911\n",
      "time 5062.673492431641\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 5 loss 3.4715023040771484\n",
      "time 5339.996472120285\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 5 loss 3.6548893451690674\n",
      "time 5617.370572090149\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 6 loss 3.3690578937530518\n",
      "time 6003.82057762146\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 6 loss 3.4157586097717285\n",
      "time 6279.279807329178\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 6 loss 3.5976595878601074\n",
      "time 6550.988702774048\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 7 loss 3.324676752090454\n",
      "time 6925.248465538025\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 7 loss 3.4247608184814453\n",
      "time 7195.00435423851\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 7 loss 3.536947011947632\n",
      "time 7463.46612071991\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 8 loss 3.3255934715270996\n",
      "time 7837.705760478973\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 8 loss 3.3456077575683594\n",
      "time 8105.9543924331665\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 8 loss 3.528752326965332\n",
      "time 8374.194128513336\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 9 loss 3.302360773086548\n",
      "time 8748.252387285233\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 9 loss 3.3268487453460693\n",
      "time 9016.798590421677\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 9 loss 3.3767762184143066\n",
      "time 9285.150767564774\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 10 loss 3.2426910400390625\n",
      "time 9659.047522068024\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 10 loss 3.221118688583374\n",
      "time 9927.577122926712\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 10 loss 3.4099862575531006\n",
      "time 10196.447468996048\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 11 loss 3.171668767929077\n",
      "time 10570.102106809616\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 11 loss 3.141671895980835\n",
      "time 10831.676782369614\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 11 loss 3.3275835514068604\n",
      "time 10925.280312299728\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 12 loss 3.0824928283691406\n",
      "time 11001.461172819138\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 12 loss 3.090184211730957\n",
      "time 11056.10242676735\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 12 loss 3.2187955379486084\n",
      "time 11110.763105154037\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 13 loss 3.0921390056610107\n",
      "time 11186.892409801483\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 13 loss 3.033048629760742\n",
      "time 11241.538620471954\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 13 loss 3.1909849643707275\n",
      "time 11295.983607053757\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 14 loss 3.0978879928588867\n",
      "time 11371.855664730072\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 14 loss 3.022150754928589\n",
      "time 11426.482285022736\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 14 loss 3.1162056922912598\n",
      "time 11481.08051943779\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 15 loss 2.9878902435302734\n",
      "time 11556.85538482666\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 15 loss 2.958552598953247\n",
      "time 11611.571391820908\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 15 loss 3.131295919418335\n",
      "time 11666.191144227982\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 16 loss 2.9550716876983643\n",
      "time 11742.146548748016\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 16 loss 2.9164419174194336\n",
      "time 11796.92129611969\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 16 loss 3.112363338470459\n",
      "time 11851.34167098999\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 17 loss 2.933232545852661\n",
      "time 11927.237939596176\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 17 loss 2.928152322769165\n",
      "time 11981.892853975296\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 17 loss 3.03285551071167\n",
      "time 12036.34401679039\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 18 loss 2.8250343799591064\n",
      "time 12112.315107822418\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 18 loss 2.8759419918060303\n",
      "time 12166.95221710205\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 18 loss 2.9450621604919434\n",
      "time 12221.612940073013\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 19 loss 2.9017720222473145\n",
      "time 12297.595406532288\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 19 loss 2.849630117416382\n",
      "time 12352.277612686157\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 19 loss 2.946455717086792\n",
      "time 12406.905469417572\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 20 loss 2.865711212158203\n",
      "time 12482.853063821793\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 20 loss 2.7369236946105957\n",
      "time 12537.526314020157\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 20 loss 2.9227287769317627\n",
      "time 12592.145003318787\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 21 loss 2.942495107650757\n",
      "time 12668.061028718948\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 21 loss 2.805955171585083\n",
      "time 12722.569063663483\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 21 loss 2.829463481903076\n",
      "time 12777.258879184723\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 22 loss 2.7943906784057617\n",
      "time 12853.30417227745\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 22 loss 2.7738988399505615\n",
      "time 12907.817836523056\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 22 loss 2.837999105453491\n",
      "time 12962.522870779037\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 23 loss 2.786637783050537\n",
      "time 13038.916821479797\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 23 loss 2.789557456970215\n",
      "time 13093.777260780334\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 23 loss 2.8432602882385254\n",
      "time 13148.325671195984\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 24 loss 2.7501511573791504\n",
      "time 13224.200406074524\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 24 loss 2.776883602142334\n",
      "time 13278.997874259949\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 24 loss 2.891289710998535\n",
      "time 13333.374601840973\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 25 loss 2.6801562309265137\n",
      "time 13409.366656780243\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 25 loss 2.7293126583099365\n",
      "time 13463.949440002441\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 25 loss 2.885864734649658\n",
      "time 13518.450922250748\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 26 loss 2.7755844593048096\n",
      "time 13594.347711801529\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 26 loss 2.8093466758728027\n",
      "time 13648.896125793457\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 26 loss 2.879410743713379\n",
      "time 13703.475868940353\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 27 loss 2.786916732788086\n",
      "time 13779.691184282303\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 27 loss 2.739121913909912\n",
      "time 13834.366427183151\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 27 loss 2.92270827293396\n",
      "time 13889.090576171875\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 28 loss 2.8330368995666504\n",
      "time 13965.110803604126\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 28 loss 2.794161558151245\n",
      "time 14019.825214862823\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 28 loss 2.9128074645996094\n",
      "time 14074.265134572983\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 29 loss 2.722156524658203\n",
      "time 14150.229894399643\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 29 loss 2.78232479095459\n",
      "time 14204.785379648209\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 29 loss 2.962634325027466\n",
      "time 14259.28840637207\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 30 loss 2.774101972579956\n",
      "time 14335.283495903015\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 30 loss 2.82334041595459\n",
      "time 14390.094337463379\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 30 loss 2.9697422981262207\n",
      "time 14444.662808418274\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 31 loss 2.8573968410491943\n",
      "time 14520.727363586426\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 31 loss 2.760178327560425\n",
      "time 14575.31927037239\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 31 loss 3.053825855255127\n",
      "time 14633.016493558884\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 32 loss 2.8588497638702393\n",
      "time 14708.131745576859\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 32 loss 2.802800416946411\n",
      "time 14762.264358758926\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 32 loss 3.0791327953338623\n",
      "time 14816.286104679108\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 33 loss 2.831829309463501\n",
      "time 14892.711681842804\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 33 loss 2.8039562702178955\n",
      "time 14946.56656718254\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 33 loss 3.074106216430664\n",
      "time 15000.502007484436\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 34 loss 2.919771909713745\n",
      "time 15075.603874206543\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 34 loss 2.8826634883880615\n",
      "time 15129.57938337326\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 34 loss 3.0477399826049805\n",
      "time 15183.336876392365\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 35 loss 2.928504228591919\n",
      "time 15258.3833963871\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 35 loss 2.879066228866577\n",
      "time 15312.237164735794\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 35 loss 3.074679374694824\n",
      "time 15366.150070667267\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 36 loss 2.9281811714172363\n",
      "time 15441.106602191925\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 36 loss 2.8755321502685547\n",
      "time 15495.111664772034\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 36 loss 3.015585422515869\n",
      "time 15548.792743206024\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 37 loss 2.978912591934204\n",
      "time 15623.701248884201\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 37 loss 2.8954837322235107\n",
      "time 15677.48804116249\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 37 loss 3.12031626701355\n",
      "time 15731.333204507828\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 38 loss 2.9045279026031494\n",
      "time 15806.01394534111\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 38 loss 2.8022212982177734\n",
      "time 15859.916120290756\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 38 loss 3.0777292251586914\n",
      "time 15913.777312994003\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 39 loss 2.875141143798828\n",
      "time 15988.703757762909\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 39 loss 2.849379301071167\n",
      "time 16042.728501796722\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 39 loss 2.98793363571167\n",
      "time 16096.666473150253\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 40 loss 2.804328203201294\n",
      "time 16171.579402685165\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 40 loss 2.8530662059783936\n",
      "time 16225.585228681564\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 40 loss 3.1340548992156982\n",
      "time 16279.62600016594\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 41 loss 2.8799803256988525\n",
      "time 16354.55363702774\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 41 loss 2.9549851417541504\n",
      "time 16408.63368868828\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 41 loss 3.0071277618408203\n",
      "time 16462.548789024353\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 42 loss 2.8648645877838135\n",
      "time 16537.78966331482\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 42 loss 2.8533053398132324\n",
      "time 16591.775828123093\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 42 loss 3.1733343601226807\n",
      "time 16645.42943406105\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 43 loss 2.836170196533203\n",
      "time 16720.466233968735\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 43 loss 2.8370816707611084\n",
      "time 16774.30184316635\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 43 loss 3.0789475440979004\n",
      "time 16828.203723669052\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 44 loss 2.834721326828003\n",
      "time 16903.415928840637\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 44 loss 2.770524263381958\n",
      "time 16957.18509554863\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 44 loss 3.0773518085479736\n",
      "time 17011.16074848175\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 45 loss 2.7882697582244873\n",
      "time 17086.19366455078\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 45 loss 2.8304662704467773\n",
      "time 17140.11715912819\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 45 loss 3.0140488147735596\n",
      "time 17193.911249637604\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 46 loss 2.757628917694092\n",
      "time 17268.940509796143\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 46 loss 2.8676130771636963\n",
      "time 17322.91308259964\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 46 loss 2.998837471008301\n",
      "time 17376.893888235092\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 47 loss 2.795400381088257\n",
      "time 17451.924448013306\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 47 loss 2.9072864055633545\n",
      "time 17505.723725795746\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 47 loss 2.977097988128662\n",
      "time 17559.5415391922\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 48 loss 2.814723491668701\n",
      "time 17634.304737329483\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 48 loss 2.826789379119873\n",
      "time 17688.317508220673\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 48 loss 3.0158345699310303\n",
      "time 17741.910172462463\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 49 loss 2.8067243099212646\n",
      "time 17816.661937475204\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 49 loss 2.7700634002685547\n",
      "time 17870.503541707993\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 49 loss 3.0294437408447266\n",
      "time 17924.082737207413\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 50 loss 2.7844173908233643\n",
      "time 17998.972762823105\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 50 loss 2.770968198776245\n",
      "time 18053.052284002304\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 50 loss 3.014052152633667\n",
      "time 18106.847635030746\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 51 loss 2.7653305530548096\n",
      "time 18181.37920475006\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 51 loss 2.8035378456115723\n",
      "time 18235.30567765236\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 51 loss 2.9607560634613037\n",
      "time 18289.167870759964\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 52 loss 2.7221953868865967\n",
      "time 18364.186055898666\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 52 loss 2.7688238620758057\n",
      "time 18417.936089992523\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 52 loss 2.9875340461730957\n",
      "time 18471.447373867035\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 53 loss 2.7694294452667236\n",
      "time 18546.236477136612\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 53 loss 2.7531397342681885\n",
      "time 18600.208702802658\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 53 loss 2.9056267738342285\n",
      "time 18654.154944896698\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 54 loss 2.7271482944488525\n",
      "time 18729.116540670395\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 54 loss 2.7036781311035156\n",
      "time 18783.037652015686\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 54 loss 2.9772214889526367\n",
      "time 18836.824735164642\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 55 loss 2.7617790699005127\n",
      "time 18911.661499500275\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 55 loss 2.729635715484619\n",
      "time 18965.680941581726\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 55 loss 2.943145751953125\n",
      "time 19019.387495994568\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 56 loss 2.745422840118408\n",
      "time 19094.264521360397\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 56 loss 2.7845981121063232\n",
      "time 19148.065729379654\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 56 loss 2.8743064403533936\n",
      "time 19201.97393345833\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 57 loss 2.750340700149536\n",
      "time 19276.95017504692\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 57 loss 2.850710153579712\n",
      "time 19330.713969945908\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 57 loss 2.83632493019104\n",
      "time 19384.55522084236\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 58 loss 2.760936975479126\n",
      "time 19459.6077709198\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 58 loss 2.7345449924468994\n",
      "time 19513.535099744797\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 58 loss 2.8934078216552734\n",
      "time 19567.458637952805\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 59 loss 2.707252025604248\n",
      "time 19642.289943933487\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 59 loss 2.743082046508789\n",
      "time 19695.99499464035\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 59 loss 2.8824541568756104\n",
      "time 19749.824195861816\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 60 loss 2.7654852867126465\n",
      "time 19824.720512151718\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 60 loss 2.728616952896118\n",
      "time 19878.70699238777\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 60 loss 2.871443271636963\n",
      "time 19932.5557949543\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 61 loss 2.684948205947876\n",
      "time 20007.519926786423\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 61 loss 2.6767776012420654\n",
      "time 20061.30859565735\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 61 loss 2.83427357673645\n",
      "time 20115.10497069359\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 62 loss 2.716912031173706\n",
      "time 20190.132295131683\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 62 loss 2.607203722000122\n",
      "time 20244.044169187546\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 62 loss 2.840702772140503\n",
      "time 20297.749736070633\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 63 loss 2.6581919193267822\n",
      "time 20372.622937440872\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 63 loss 2.7380948066711426\n",
      "time 20422.10539984703\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 63 loss 2.8218891620635986\n",
      "time 20475.084505081177\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 64 loss 2.66947603225708\n",
      "time 20549.449088811874\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 64 loss 2.6900041103363037\n",
      "time 20603.345522880554\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 64 loss 2.8475182056427\n",
      "time 20657.24914765358\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 65 loss 2.5937764644622803\n",
      "time 20732.323580265045\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 65 loss 2.732888698577881\n",
      "time 20786.16972899437\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 65 loss 2.8532299995422363\n",
      "time 20840.135981321335\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 66 loss 2.6700868606567383\n",
      "time 20915.085216999054\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 66 loss 2.6747660636901855\n",
      "time 20969.091727495193\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 66 loss 2.8605470657348633\n",
      "time 21022.88250017166\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 67 loss 2.63187313079834\n",
      "time 21097.804460525513\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 67 loss 2.7128679752349854\n",
      "time 21147.147605657578\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 67 loss 2.781257152557373\n",
      "time 21200.329657793045\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 68 loss 2.6026546955108643\n",
      "time 21274.219235420227\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 68 loss 2.664815664291382\n",
      "time 21327.683537960052\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 68 loss 2.8131778240203857\n",
      "time 21380.912540912628\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 69 loss 2.6450321674346924\n",
      "time 21454.838668107986\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 69 loss 2.736189603805542\n",
      "time 21508.035588026047\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 69 loss 2.869889497756958\n",
      "time 21561.012031316757\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 70 loss 2.6481773853302\n",
      "time 21634.855314016342\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 70 loss 2.6183114051818848\n",
      "time 21688.054993391037\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 70 loss 2.863368511199951\n",
      "time 21741.20520734787\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 71 loss 2.60294508934021\n",
      "time 21815.31914138794\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 71 loss 2.734912633895874\n",
      "time 21868.44588494301\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 71 loss 2.867384195327759\n",
      "time 21921.665273666382\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 72 loss 2.7052648067474365\n",
      "time 21995.73984646797\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 72 loss 2.686326026916504\n",
      "time 22048.882133245468\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 72 loss 2.867861032485962\n",
      "time 22102.058530330658\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 73 loss 2.532071828842163\n",
      "time 22176.133818864822\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 73 loss 2.6957614421844482\n",
      "time 22229.34777379036\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 73 loss 2.8393757343292236\n",
      "time 22282.28580379486\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 74 loss 2.6151318550109863\n",
      "time 22356.316098451614\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 74 loss 2.737410068511963\n",
      "time 22409.55550289154\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 74 loss 2.902811288833618\n",
      "time 22463.528572797775\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 75 loss 2.593782424926758\n",
      "time 22538.26201748848\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 75 loss 2.6957743167877197\n",
      "time 22591.492971658707\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 75 loss 2.8598785400390625\n",
      "time 22644.627513170242\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 76 loss 2.6209137439727783\n",
      "time 22718.555881500244\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 76 loss 2.6723077297210693\n",
      "time 22771.870873451233\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 76 loss 2.9134199619293213\n",
      "time 22824.892524003983\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 77 loss 2.6380622386932373\n",
      "time 22898.847282409668\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 77 loss 2.7217490673065186\n",
      "time 22952.115349531174\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 77 loss 2.9176182746887207\n",
      "time 23005.316641807556\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 78 loss 2.538865089416504\n",
      "time 23079.4305832386\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 78 loss 2.7448296546936035\n",
      "time 23132.564225673676\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 78 loss 2.860903739929199\n",
      "time 23185.740618944168\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 79 loss 2.668558359146118\n",
      "time 23259.540374279022\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 79 loss 2.714928388595581\n",
      "time 23312.613913059235\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 79 loss 2.905745029449463\n",
      "time 23365.632578372955\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 80 loss 2.6630101203918457\n",
      "time 23439.527306079865\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 80 loss 2.7332561016082764\n",
      "time 23492.843786001205\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 80 loss 2.9049079418182373\n",
      "time 23545.87756729126\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 81 loss 2.6218883991241455\n",
      "time 23619.819873809814\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 81 loss 2.703836679458618\n",
      "time 23673.03309893608\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 81 loss 2.8277008533477783\n",
      "time 23726.15708589554\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 82 loss 2.5936880111694336\n",
      "time 23800.16837120056\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 82 loss 2.6608288288116455\n",
      "time 23853.214623212814\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 82 loss 2.8791074752807617\n",
      "time 23906.12324166298\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 83 loss 2.5004489421844482\n",
      "time 23980.021894693375\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 83 loss 2.6731984615325928\n",
      "time 24033.250463724136\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 83 loss 2.8897650241851807\n",
      "time 24086.472541332245\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 84 loss 2.5040550231933594\n",
      "time 24160.35730957985\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 84 loss 2.685154914855957\n",
      "time 24213.698654174805\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 84 loss 2.8699378967285156\n",
      "time 24266.86308670044\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 85 loss 2.598372220993042\n",
      "time 24340.994071245193\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 85 loss 2.691877603530884\n",
      "time 24394.40847158432\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 85 loss 2.8263752460479736\n",
      "time 24447.535479068756\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 86 loss 2.5363810062408447\n",
      "time 24521.301984786987\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 86 loss 2.68822979927063\n",
      "time 24574.55179619789\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 86 loss 2.9001755714416504\n",
      "time 24627.502568244934\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 87 loss 2.624936819076538\n",
      "time 24701.207552671432\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 87 loss 2.749056339263916\n",
      "time 24754.224658966064\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 87 loss 2.91378116607666\n",
      "time 24807.45467376709\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 88 loss 2.6569344997406006\n",
      "time 24881.40993618965\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 88 loss 2.7189247608184814\n",
      "time 24934.584493637085\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 88 loss 2.9357705116271973\n",
      "time 24987.699869394302\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 89 loss 2.5153586864471436\n",
      "time 25061.415097236633\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 89 loss 2.660872459411621\n",
      "time 25114.54321885109\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 89 loss 2.9287807941436768\n",
      "time 25167.472939491272\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 90 loss 2.604436159133911\n",
      "time 25241.3949508667\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 90 loss 2.708386182785034\n",
      "time 25294.497755765915\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 90 loss 2.8159847259521484\n",
      "time 25347.527737617493\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 91 loss 2.5384011268615723\n",
      "time 25421.582578659058\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 91 loss 2.7654683589935303\n",
      "time 25474.733448266983\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 91 loss 2.8976492881774902\n",
      "time 25527.697931289673\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 92 loss 2.6151180267333984\n",
      "time 25601.774528503418\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 92 loss 2.7331926822662354\n",
      "time 25655.00802254677\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 92 loss 2.8442530632019043\n",
      "time 25708.078012943268\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 93 loss 2.58390736579895\n",
      "time 25782.03291273117\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 93 loss 2.691080331802368\n",
      "time 25835.30642390251\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 93 loss 2.8380074501037598\n",
      "time 25888.441118478775\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 94 loss 2.496880292892456\n",
      "time 25962.283787488937\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 94 loss 2.680885076522827\n",
      "time 26015.493547201157\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 94 loss 2.8820416927337646\n",
      "time 26068.52755522728\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 95 loss 2.5358150005340576\n",
      "time 26142.555056333542\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 95 loss 2.6811585426330566\n",
      "time 26195.617418050766\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 95 loss 2.8885140419006348\n",
      "time 26248.72076201439\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 96 loss 2.4831607341766357\n",
      "time 26322.76028776169\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 96 loss 2.6433539390563965\n",
      "time 26375.892911195755\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 96 loss 2.915158987045288\n",
      "time 26429.056924819946\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 97 loss 2.4673192501068115\n",
      "time 26503.006710767746\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 97 loss 2.7186195850372314\n",
      "time 26556.3014960289\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 97 loss 2.884533166885376\n",
      "time 26609.425970315933\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 98 loss 2.5747077465057373\n",
      "time 26683.542731046677\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 98 loss 2.6291279792785645\n",
      "time 26736.672917366028\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 98 loss 2.8227152824401855\n",
      "time 26789.682081222534\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "epoch 99 loss 2.5580551624298096\n",
      "time 26863.605778455734\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.6\n",
      "1.7\n",
      "1.8\n",
      "1.9\n",
      "epoch 99 loss 2.7028119564056396\n",
      "time 26916.917786598206\n",
      "2.0\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "2.9\n",
      "epoch 99 loss 2.849508285522461\n",
      "time 26970.009341955185\n",
      "3.0\n",
      "3.1\n",
      "3.2\n",
      "3.3\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1b54981e",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# %matplotlib qt\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a9a7b4cd",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# with open('loss.txt','r',encoding='utf-8') as f:\n",
    "#     data=f.read()\n",
    "# data=eval(data)\n",
    "# fig=plt.figure()\n",
    "# plt.plot([i*100 for i in range(len(data))],data)\n",
    "# plt.xlabel('batch_num')\n",
    "# plt.ylabel('loss')\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ede3b1",
   "metadata": {},
   "source": [
    "## 20 加载之前的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b1d09fc",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# en_path = r'H:\\datasets\\data\\翻译1\\test.en.txt'\n",
    "# ch_path = r'H:\\datasets\\data\\翻译1\\test.ch.txt'\n",
    "# tokenizer = Tokenizer(en_path, ch_path, count_min=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "51ea194b",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 数据评估\n",
    "def eval1():\n",
    "    device='cuda'\n",
    "    model1 = Transformer(tokenizer)\n",
    "    model1.load_state_dict(torch.load(f'./model/translation_25.pt'))\n",
    "    model1 = model1.to(device)\n",
    "    model1.eval()\n",
    "    all_=[]\n",
    "    for i in range(100):\n",
    "        random_integers = range(len(tokenizer.test))[i*10:i*10+10]  # 评估\n",
    "        end=compute_bleu4(tokenizer, random_integers, model1, device)\n",
    "        if end==0:\n",
    "            continue\n",
    "        all_.append(end)\n",
    "    print(sum(all_)/len(all_)) # 输出bleu4得分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9dff5b29",
   "metadata": {
    "metadata": {},
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './model/translation_25.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43meval1\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 5\u001b[0m, in \u001b[0;36meval1\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m model1 \u001b[38;5;241m=\u001b[39m Transformer(tokenizer)\n\u001b[0;32m----> 5\u001b[0m model1\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./model/translation_25.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      6\u001b[0m model1 \u001b[38;5;241m=\u001b[39m model1\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m model1\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/anaconda3/envs/prml/lib/python3.8/site-packages/torch/serialization.py:579\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    577\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    583\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    584\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/anaconda3/envs/prml/lib/python3.8/site-packages/torch/serialization.py:230\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 230\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/anaconda3/envs/prml/lib/python3.8/site-packages/torch/serialization.py:211\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_file, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './model/translation_25.pt'"
     ]
    }
   ],
   "source": [
    "eval1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73be28b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
